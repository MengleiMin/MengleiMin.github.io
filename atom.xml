<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Memicat&#39;s blog!</title>
  
  <subtitle>Work rationally, life emotionally!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://memicat.com/"/>
  <updated>2018-03-06T22:28:29.962Z</updated>
  <id>http://memicat.com/</id>
  
  <author>
    <name>Memicat</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于贝叶斯推理的时序趋势改变检测</title>
    <link href="http://memicat.com/2018/03/05/2018.03.05_%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E7%90%86%E7%9A%84%E6%97%B6%E5%BA%8F%E8%B6%8B%E5%8A%BF%E6%94%B9%E5%8F%98%E6%A3%80%E6%B5%8B/"/>
    <id>http://memicat.com/2018/03/05/2018.03.05_基于贝叶斯推理的时序趋势改变检测/</id>
    <published>2018-03-04T23:00:00.000Z</published>
    <updated>2018-03-06T22:28:29.962Z</updated>
    
    <content type="html"><![CDATA[<p>This paper elaborates a Bayesian method to estimate the location of the singularities and to produce some confidence intervals. And then validate the ability and sensitivity of the inference method.<br><a id="more"></a></p><h3 id="1-Formulation-of-the-linear-mixed-model"><a href="#1-Formulation-of-the-linear-mixed-model" class="headerlink" title="1. Formulation of the linear mixed model"></a>1. Formulation of the linear mixed model</h3><p>This paper proposes two aspects of change points in a time series：</p><ol><li>A change point is commonly associated with a sudden change of local trend in the data.</li><li>Systematic evolution of the local variability around its average value undergoes a sudden transition at the change point.</li></ol><p>Above two aspects of change point can be combined into a <strong>Linear Mixed Model(LMM)</strong> with hyperparameters.</p><p>The simplest type of signal undergoing a change point at time θ can be expressed as:</p><script type="math/tex; mode=display">\begin{equation}y(t)=\beta_0+\beta_1{|\theta-t|}_\_+\beta_2{|\theta-t|}_++ξ(t).\end{equation}</script><p>Use the elementary Hockey sticks of first order defined through:</p><script type="math/tex; mode=display">\begin{equation}|\theta-t|_\_=(\zeta_\_^\theta)=\begin{cases}\theta-t,& \text{if $t \le \theta$} \\0,& \text{else}\end{cases},\end{equation}</script><script type="math/tex; mode=display">\begin{equation}|\theta-t|_+=(\zeta_+^\theta)=\begin{cases}\theta-t,& \text{if $t \ge \theta$} \\0,& \text{else}\end{cases}.\end{equation}</script><p> <strong>Random fluctuations</strong> can be caused by <strong>measurement noise</strong> and some <strong>intrinsic variability</strong>, which is not captured by the low dimensional mean dynamics on both sides of the change point θ. So here we consider stochastic fluctuations ξ(t) whose amplitudes undergo a transition themselves according to:</p><script type="math/tex; mode=display"> \begin{equation}STD(ξ(t))=\sigma(1+s_1{|t-\theta|}_\_+s_2{|t-\theta|}_+). \end{equation}</script><p> The scale factor σ could be the level of the measurement noise or some background level of the intrinsic fluctuations, whereas the constants s1,2 describe the systematic evolution of the models intrinsic variability prior and after the change point measured in units of σ. Assume the Gaussian random variables, different time points are uncorrelated (It is not ture in practice).<br> <img src="/images/2018.3.05_1.png" alt=""><br>Given a data set of n time points $t_i, i = 1,…,n$, the observation vector $ \vec {\mkern1mu y} = [s(t_i)]^t ∈ R^n$ can be written as follows:</p><script type="math/tex; mode=display">\begin{equation}\vec{\mkern1mu y} =\vec{\mkern1mu F}\vec{\mkern1mu \beta}+\vec{ ξ}\end{equation}</script><p>$\beta = (\beta_0,\beta_1,\beta_2)^T \in R^3$ is the fixed effect vector, corresponding to the coefficients of the LMM for mean behavior.$F \in R^3$ is the system matrix of the fixed effects.</p><script type="math/tex; mode=display">\begin{equation}F_\theta=\left(  \begin{array}{ccc}  1 & {(\zeta_\_^\theta)}_1& {(\zeta_+^\theta)}_1 \\  \vdots & \vdots &  \vdots \\  1 & {(\zeta_\_^\theta)}_n &  {(\zeta_+^\theta)}_n \\  \end{array}\right)\end{equation}</script><p>And noise $ξ ∈ R^n$ is a Gaussian random vector with zero mean and covariance matrix $σ^2Ω ∈ R^{n×n}$,</p><script type="math/tex; mode=display">\begin{equation}ξ \sim N（0,\sigma^2 \Omega).\end{equation}</script><p>The covariance itself is structured noise parametrized by two slope parameters $s = (s_1,s_2)$ and the change point θ itself as:</p><script type="math/tex; mode=display">\begin{equation}{(\Omega_{\theta,s_1,s_2})}_{ij}=([1+s_1{(\zeta_\_^\theta)}_j+s_2{(\zeta_+^\theta)}_j]^2))\times \delta_{ij}\end{equation}</script><p>So the probability density of the observations for fixed parameters (i.e. fixed effects, change point, slope parameters) can be written as</p><script type="math/tex; mode=display">\begin{equation}\vec{\mkern1mu y} \sim N(\vec{\mkern1mu F}\hat{\mkern1mu \beta},\sigma^2\Omega).\end{equation}</script><p>The Likelihood function of the parameters given the data can then be written as:</p><script type="math/tex; mode=display">\begin{equation}\mathcal{L}（\beta,\sigma,s,\theta|y） = \frac{1}{(2\pi\sigma^2)^{n/2}\sqrt{|\Omega|}}e^{-\frac{1}{2\sigma^2}(y-F\beta)^T\Omega^{-1}(y-F\beta)}\end{equation}</script><h3 id="2-Bayesian-inversion"><a href="#2-Bayesian-inversion" class="headerlink" title="2. Bayesian inversion"></a>2. Bayesian inversion</h3><p>Assume a priori no correlations between the parameters, the joint prior distribution can be factorized into the independent parts:</p><script type="math/tex; mode=display">\begin{equation}p(\beta,\sigma,\theta,s)=p(\theta)\cdotp(\sigma)\cdotp(s)\cdotp(\beta).\end{equation}\\.\\.\\.</script><script type="math/tex; mode=display">\begin{equation}p(\sigma,\theta,\boldsymbol{s|y}) \sim \frac{\sigma^{1-n}}{\sqrt{|\Omega||F^T\Omega^{-1}F|}} e^{^{-\frac{1}{2\sigma^2}R^2}}\end{equation}</script><p>and</p><script type="math/tex; mode=display">\begin{equation}p(\beta,\theta,\boldsymbol{s|y}) \sim \frac{[(y-F\beta)^T\Omega^{-1}(y-F\beta)]^{-n/2}}{\sqrt{|\Omega|}}\end{equation}</script><p>Further marginalization may be performed to yield</p><script type="math/tex; mode=display">\begin{equation}\begin{split}p(\theta,s|y)&= \int d\sigma d\beta p(\beta,\sigma,\theta,s|y)\\&=C^\prime \cdot \frac{R^{-(n-2)}}{\sqrt{|\Omega||F^T\Omega^{-1}F|}}.\end{split}\end{equation}</script><p>$C^\prime$ is a constant. Finally the posterior marginal distribution of θ can be computed by numeric evaluation of the following integral:</p><script type="math/tex; mode=display">\begin{equation}p(\theta|\boldsymbol{y}) = \int d\boldsymbol{s} p(\theta,\boldsymbol{s|y}).\end{equation}</script><p>In the same way the numeric θ integral may be performed to elaborate the posterior information about the involved slope parameters s of the heteroscedastic behavior around the change point:</p><script type="math/tex; mode=display">\begin{equation}p(\boldsymbol{s|y}) = \int d\theta p(\theta,\boldsymbol{s|y}).\end{equation}</script><h3 id="3-Local-posterior-density"><a href="#3-Local-posterior-density" class="headerlink" title="3. Local posterior density"></a>3. Local posterior density</h3><p>This paper proposes a kernel based local posterior method. Around each time point t we choose a data window <script type="math/tex">I_t = [t− T/2,t+ T/2 ]</script> of length T. Inside this window, we take as prior distribution for the change point location p(θ) a flat prior inside some subinterval of length a:</p><script type="math/tex; mode=display">\begin{equation}p(\theta)=\begin{cases}1/a,& \text{for $t-\frac{a}{2} \le \theta \le t+\frac{a}{2}$} \\0,& \text{else}\end{cases},0<a<T.\end{equation}</script><p>We then compute the local posterior <script type="math/tex">p_t(θ|y_{|I_t})</script> around t based on the subseries in the data window <script type="math/tex">y_{|I_t}</script>. This yields a posterior distribution of a possible change point within each window under the assumption that there is actually a singularity within the window.<br>……After lots of inference step, we will use a plain Bayes factor BF.<br>For example,to infer on the true change point values $(θ_1,θ_2,θ_3)$ = (40,100,160) via the estimators $(\hat{\mkern1mu \theta_1} , \hat{\mkern1mu \theta_2} , θ\hat{\mkern1mu \theta_3} ) = (38.9, 93.0, 162.9)$ within their intervals ([33.9, 47.8], [87.5, 109.1], [158.9, 167.0]) of about 90% confidence.</p><p><img src="/images/2018.3.05_4.png" alt="" title="Fig.2 Normalized sum of local posterior densities weighted by the Bayes factor, computed for sub time series of nsub = 50 data points and a sampling grid of ncp = 30 change points."></p><h3 id="4-Validation-the-method"><a href="#4-Validation-the-method" class="headerlink" title="4. Validation the method"></a>4. Validation the method</h3><p>Demonstrate the technique by applying it on a time series including a known significant change point. For this purpose we analyze the annual Nile River flow measured at Aswan from 1871 to 1970 [27]. And the year 1899 has been verified as the change point. So we want to verify the previous model by this dataset.<br>First of all we compute the global posterior density p(θ,s|y) as presented in Eq.(15). By initially guessing a reasonable sampling grid for the change point θ and the slope parameters s from the data, we clearly obtain significant maxima in the posterior projections p(θ|y) and p(s|y). And adjust the sampling to obatain finner posterior structures. Finally get the estimator the change point as <script type="math/tex">\hat{\mkern1mu \theta} = 1898</script> within a confidence interval [1895,1901] of over 95%. The slope parameters of the deviation are estimated as <script type="math/tex">(\hat{\mkern1mu s_1},\hat{\mkern1mu s_2}) = (0.0065, −0.0015)</script> within the 90% confidence intervals $\hat{\mkern1mu s_1}$ in [−0.0190, 0.0450] and$ \hat{\mkern1mu s_2}$ in [−0.0065, 0.0855].<br>Then use the estimator $\hat{\mkern1mu \theta}$ and $\hat{\mkern1mu \sigma}$ to compute the posterior projections p(β,θ,s|y) and p(σ,θ,s|y) formulated in Eq.(12) and (13), and then estimate the remaining parameters $\beta$ and $\sigma$. Finally we reveal from the global posterior distribution the most probable model plotted in Fig.2 and listed in Tab.II.<br> <img src="/images/2018.3.05_2.png" alt="" title="Figure 3: Annual Nile flow containing a known change point at θ = 1899. The sum of localized posterior densities weighted with respect to the Bayes factor BF indicates a change point at $\hat{\mkern1mu \theta}$ = 1898 within its confidence interval [1896, 1900] of about 90%. The estimated underlying model reveals the most dominant transition in the behavior of the mean."><br><img src="/images/2018.3.05_3.png" alt=""><br>Since most secondary maxima are &lt; 1% we ignore them and therefore conclude on one global change point at $\hat{\mkern1mu \theta}$ = 1898 in the interval [1896, 1900] of about 90% confidence.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>Schütz N, Holschneider M. Detection of trend changes in time series using Bayesian inference[J]. Physical Review E, 2011, 84(2): 021120.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This paper elaborates a Bayesian method to estimate the location of the singularities and to produce some confidence intervals. And then validate the ability and sensitivity of the inference method.&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="http://memicat.com/categories/Data-Mining/"/>
    
    
      <category term="Trend change detection" scheme="http://memicat.com/tags/Trend-change-detection/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘基本算法</title>
    <link href="http://memicat.com/2018/03/04/2018.03.04_%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/"/>
    <id>http://memicat.com/2018/03/04/2018.03.04_数据挖掘基本算法/</id>
    <published>2018-03-03T23:00:00.000Z</published>
    <updated>2018-03-06T17:32:26.401Z</updated>
    
    <content type="html"><![CDATA[<p>此篇主要记录自己在学习数据挖掘过程中遇到的一些基本算法，目的是做一个整理，方便自己随时查阅。由于这些算法已经比较普及了，所以为了节省时间，大多参考网上前人的解释。<br><a id="more"></a></p><h3 id="1-K-means-1"><a href="#1-K-means-1" class="headerlink" title="1. K-means [1]"></a>1. K-means [1]</h3><h4 id="1-1-算法原理"><a href="#1-1-算法原理" class="headerlink" title="1.1 算法原理"></a>1.1 算法原理</h4><ol><li>随机选择k个中心点（无监督学习）</li><li>遍历所有的数据，将每个数据划分到离它最近的中心点中（根据Euclidean distance（欧几里得距离）进行划分，也就是计算遍历的数据点离所有中心点的距离，然后选择最小的那一个）</li><li>计算每个聚类的平均值，作为聚类的新中心点。</li><li>重复2-3步骤，直到这k个中心点不再变化，即收敛了，或者迭代重复的次数达到了用户的规定值<br>Matlab code：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[IDX,C,sumd,D] = kmeans(X,k,&apos;distance&apos;,&apos;sqEuclidean&apos;,&apos;start&apos;,&apos;sample&apos;)</span><br></pre></td></tr></table></figure></li></ol><h4 id="1-2-算法优化"><a href="#1-2-算法优化" class="headerlink" title="1.2 算法优化"></a>1.2 算法优化</h4><p>由于中心点的随机选择，导致聚类的不确定性，故需要方法来优化。<br>Way one：重复执行几次此算法，选择sum-distance最小的一次作为聚类结果。<br>Way two：重复执行几次此算法，选取轮廓系数最小的一次作为聚类结果。<br>轮廓系数：结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果。该值处于-1~1之间，值越大，表示聚类效果越好。</p><h4 id="1-3-k的选取"><a href="#1-3-k的选取" class="headerlink" title="1.3 k的选取"></a>1.3 k的选取</h4><p>k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。</p><h3 id="2-KNN-K-nearest-neighbour-2"><a href="#2-KNN-K-nearest-neighbour-2" class="headerlink" title="2. KNN (K nearest neighbour)[2]"></a>2. KNN (K nearest neighbour)[2]</h3><h4 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h4><p>训练集数据和标签（label）已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类:</p><ol><li>计算测试数据与各个训练数据之间的距离，距离有两种方式：<br>欧式距离: <script type="math/tex">d(x,y)=\sqrt{\sum_{k=1}^{n}(x_k-y_k)^2}</script><br>曼哈顿距离：<script type="math/tex">d(x,y)=\sqrt{\sum_{k=1}^n|x_k-y_k|}</script></li><li>按照距离的递增关系进行排序.</li><li>选取距离最小的K个点.</li><li>确定前K个点所在类别的出现频率.</li><li>返回前K个点中出现频率最高的类别作为测试数据的预测分类。</li></ol><p>KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成反比。</p><h3 id="3-贝叶斯（Naive-Bayes）-3"><a href="#3-贝叶斯（Naive-Bayes）-3" class="headerlink" title="3.贝叶斯（Naıve Bayes）[3]"></a>3.贝叶斯（Naıve Bayes）[3]</h3><h4 id="3-1-贝叶斯定理"><a href="#3-1-贝叶斯定理" class="headerlink" title="3.1 贝叶斯定理"></a>3.1 贝叶斯定理</h4><p>P(A|B)表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为：<script type="math/tex">P(A|B)=\frac{P(AB)}{P(B)}</script><br>我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。</p><script type="math/tex; mode=display">P(B|A)=\frac{P(A|B)P(B)}{P(A)}</script><h4 id="3-2-朴素贝叶斯分类"><a href="#3-2-朴素贝叶斯分类" class="headerlink" title="3.2 朴素贝叶斯分类"></a>3.2 朴素贝叶斯分类</h4><ol><li>设<script type="math/tex">x = {a_1,a_2,...,a_m}</script>为一个待分类项，而每个a为x的一个特征属性。</li><li>有类别集合<script type="math/tex">C = {y_1,y_2,...,y_n}</script>。</li><li>计算<script type="math/tex">P(y_1|x),P(y_2|x),...,P(y_n|x)</script>。</li><li>如果<script type="math/tex">P(y_k|x)=max\{P(y_1|x),P(y_2|x),...,P(y_n|x)\}</script>，则<script type="math/tex">x \in y_k</script>。<br>那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：<br>（1）找到一个已知分类的待分类项集合，这个集合叫做训练样本集。<br>（2）统计得到在各类别下各个特征属性的条件概率估计。即<script type="math/tex; mode=display">P(a_1|y_1）,P(a_2|y_1),...,P(a_m|y_1);P(a_1|y_2）,P(a_2|y_2),...,P(a_m|y_2);...;P(a_1|y_n）,P(a_2|y_n),...,P(a_m|y_n);</script>（3）如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：<script type="math/tex; mode=display">P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)}</script>因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：<script type="math/tex; mode=display">P(x|y_i)P(y_i)=P(a_1|y_i）P(a_2|y_i)...P(a_m|y_i)P(y_i)=P(y_i)\prod_{j=1}^m P(a_j|y_i)</script><h4 id="3-3边缘分布"><a href="#3-3边缘分布" class="headerlink" title="3.3边缘分布"></a>3.3边缘分布</h4><a href="https://baike.baidu.com/item/%E8%BE%B9%E7%BC%98%E5%88%86%E5%B8%83" target="_blank" rel="noopener">https://baike.baidu.com/item/%E8%BE%B9%E7%BC%98%E5%88%86%E5%B8%83</a></li></ol><h3 id="4-决策树（Decision-tree）-4"><a href="#4-决策树（Decision-tree）-4" class="headerlink" title="4.决策树（Decision tree）[4]"></a>4.决策树（Decision tree）[4]</h3><p>相比贝叶斯算法，决策树的优势在于构造过程不需要任何领域知识或参数设置，因此在实际应用中，对于探测式的知识发现，决策树更加适用。</p><h4 id="4-1-基本定义"><a href="#4-1-基本定义" class="headerlink" title="4.1 基本定义"></a>4.1 基本定义</h4><p>决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。</p><h4 id="4-2-决策树的构造"><a href="#4-2-决策树的构造" class="headerlink" title="4.2 决策树的构造"></a>4.2 决策树的构造</h4><p>构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况：</p><ol><li>属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。</li><li>属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。</li><li>属性是连续值。此时确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支。</li></ol><p>构造决策树的关键性内容是进行属性选择度量，属性选择度量是一种选择分裂准则，是将给定的类标记的训练集合的数据“最好”地分成个体类的启发式方法，它决定了拓扑结构及分裂点split_point的选择。属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯的贪心策略。贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的是在某种意义上的局部最优解。贪心算法对每个子问题的解决方案都做出选择，不能回退；动态规划则会根据以前的选择结果对当前进行选择，有回退功能。这里介绍ID3和C4.5两种常用算法。</p><h4 id="4-3-ID3算法"><a href="#4-3-ID3算法" class="headerlink" title="4.3 ID3算法"></a>4.3 ID3算法</h4><p>从信息论知识中我们直到，期望信息越小，信息增益越大，从而纯度越高。所以ID3算法的核心思想就是以信息增益度量属性选择，选择分裂后信息增益最大的属性进行分裂。下面先定义几个要用到的概念。<br>设D为用类别对训练元组进行的划分，则D的熵（entropy）表示为：</p><script type="math/tex; mode=display">info(D) = -\sum_{i=1}^{m}p_ilog_2(p_i)</script><p>其中$p_i$表示第i个类别在整个训练元组中出现的概率，可以用属于此类别元素的数量除以训练元组元素总数量作为估计。熵的实际意义表示是D中元组的类标号所需要的平均信息量。<br>现在我们假设将训练元组D按属性A进行划分，则A对D划分的期望信息为：</p><script type="math/tex; mode=display">info_A(D)=\sum_{j=1}^v \frac{|D_j|}{|D|}info(D_j)</script><p> 而信息增益即为两者的差值：</p><script type="math/tex; mode=display">gain(A) = info(D)-info_A(D)</script><p>ID3算法就是在每次需要分裂时，计算每个属性的增益率，然后选择增益率最大的属性进行分裂。</p><h4 id="4-4-C4-5算法"><a href="#4-4-C4-5算法" class="headerlink" title="4.4 C4.5算法"></a>4.4 C4.5算法</h4><p> ID3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。ID3的后继算法C4.5使用增益率（gain ratio）的信息增益扩充，试图克服这个偏倚（bias）.<br>C4.5算法首先定义了“分裂信息”，其定义可以表示成：</p><script type="math/tex; mode=display">split\_info_A(D) = -\sum_{j=1}^{v}\frac{|D_j|}{|D|}log_2(\frac{|D_j|}{D})</script><p>其中各符号意义与ID3算法相同，然后，增益率被定义为：</p><script type="math/tex; mode=display">gain\_radio(A)=\frac{gain(A)}{split\_info_A}</script><p>C4.5选择具有最大增益率的属性作为分裂属性.</p><h4 id="4-5-补充说明"><a href="#4-5-补充说明" class="headerlink" title="4.5 补充说明"></a>4.5 补充说明</h4><ol><li><p>如果属性用完了怎么办<br>在决策树构造过程中可能会出现这种情况：所有属性都作为分裂属性用光了，但有的子集还不是纯净集，即集合内的元素不属于同一类别。在这种情况下，由于没有更多信息可以使用了，一般对这些子集进行“多数表决”，即使用此子集中出现次数最多的类别作为此节点类别，然后将此节点作为叶子节点。</p></li><li><p>关于剪枝<br>在实际构造决策树时，通常要进行剪枝，这时为了处理由于数据中的噪声和离群点导致的过分拟合问题。剪枝有两种：<br>先剪枝——在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造。<br>后剪枝——先构造完成完整的决策树，再通过某些条件遍历树进行剪枝。</p></li><li><p>过拟合(Overfitting) [5]<br>所谓Overfitting就是指过度训练，是说机器/Model学习到的Hypothesis过度接近Training Data，而导致Testing Date的时候，Error变得更大。例如，想用高次多项式的Hypothesis，h(w)作Linear Regression</p><script type="math/tex; mode=display">h(w)=w_0x^0+w_1x^1+w_2x^2+...+w_nx^n</script><p>其中，w是weight，n表示多项式的次数(Order).<br>（1）如果多项式的Order不够大，则无法使得Hypothesis贴近Training Data。<br>（2）如果多项式的Order大小适中，则可以使得Hypothesis贴近Training Data以及Testing Date，此时<script type="math/tex">E_{in}</script>和<script type="math/tex">E_{out}</script>都会降低。<br>（3）如果多项式的Order太大，则可以使得Hypothesis过度贴近Training Data,远离Testing Date，造成<script type="math/tex">E_{out}</script>变大。</p></li></ol><h3 id="5-SVM支持向量机-Support-vector-machines-6"><a href="#5-SVM支持向量机-Support-vector-machines-6" class="headerlink" title="5. SVM支持向量机(Support vector machines) [6]"></a>5. SVM支持向量机(Support vector machines) [6]</h3><h4 id="5-1-基本介绍"><a href="#5-1-基本介绍" class="headerlink" title="5.1 基本介绍"></a>5.1 基本介绍</h4><p>(1)SVM是一种监督式学习的方法，可广泛地应用于统计分类以及回归分析。支持向量机属于一般化线性分类器，这族分类器的特点是他们能够同时最小化经验误差与最大化几何边缘区，因此支持向量机也被称为最大边缘区分类器。<br>(2）支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。</p><h4 id="5-2-算法描述"><a href="#5-2-算法描述" class="headerlink" title="5.2 算法描述"></a>5.2 算法描述</h4><p>1.假设我们要通过三八线把实心圈和空心圈分成两类。那么有无数多条线可以完成这个任务。<br>2.在SVM中，我们寻找一条最优的分界线使得它到两边的margin都最大。<br>3.在这种情况下边缘加粗的几个数据点就叫做support vector，这也是这个分类算法名字的来源。<br><img src="/images/2018.3.04_1.jpg" alt=""><br>拓展至任意n维乃至无限维空间:<br><img src="/images/2018.3.04_2.jpg" alt=""></p><h3 id="6-Apriori（先验算法）-7"><a href="#6-Apriori（先验算法）-7" class="headerlink" title="6. Apriori（先验算法）[7]"></a>6. Apriori（先验算法）[7]</h3><p>在计算机科学以及数据挖掘领域中， 先验算法（Apriori Algorithm 是关联规则学习的经典算法之一。先验算法的设计目的是为了处理包含交易信息内容的数据库（例如,顾客购买的商品清单，或者网页常访清单。）而其他的算法则是设计用来寻找无交易信息（如Winepi算法和Minepi算法）或无时间标记（如DNA测序）的数据之间的联系规则。<br>Apriori算法过程分为两个步骤：<br>第一步通过迭代，检索出事务数据库中的所有频繁项集，即支持度不低于用户设定的阈值的项集；<br>第二步利用频繁项集构造出满足用户最小信任度的规则。<br>具体做法就是：<br>首先找出频繁1-项集，记为L1；然后利用L1来产生候选项集C2，对C2中的项进行判定挖掘出L2，即频繁2-项集；不断如此循环下去直到无法发现更多的频繁k-项集为止。每挖掘一层<script type="math/tex">L_k</script>就需要扫描整个数据库一遍。算法利用了一个性质：<br>Apriori 性质：任一频繁项集的所有非空子集也必须是频繁的。意思就是说，生成一个k-itemset的候选项时，如果这个候选项有子集不在(k-1)-itemset(已经确定是frequent的)中时，那么这个候选项就不用拿去和支持度判断了，直接删除。<br>虽然先验算法具有显著的历史地位，但是其中的一些低效与权衡弊端也进而引致了许多其他的算法的产生。候选集产生过程生成了大量的子集（先验算法在每次对数据库进行扫描之前总是尝试加载尽可能多的候选集）。并且自底而上的子集浏览过程（本质上为宽度优先的子集格遍历）也直到遍历完所有$2^{|S|}-1$个可能的子集之后才寻找任意最大子集S。</p><h3 id="7-卡方分布和卡方检验（chi-square-distribution）-8"><a href="#7-卡方分布和卡方检验（chi-square-distribution）-8" class="headerlink" title="7.卡方分布和卡方检验（chi-square distribution）[8]"></a>7.卡方分布和卡方检验（chi-square distribution）[8]</h3><h4 id="7-1-卡方分布"><a href="#7-1-卡方分布" class="headerlink" title="7.1 卡方分布"></a>7.1 卡方分布</h4><p>卡方分布(chi-square distribution, χ2-distribution)是概率统计里常用的一种概率分布，也是统计推断里应用最广泛的概率分布之一，在假设检验与置信区间的计算中经常能见到卡方分布的身影。<br>我们先来看看卡方分布的定义：<br>若k个独立的随机变量<script type="math/tex">Z_1,Z_2,⋯,Z_k</script>,且符合标准正态分布N(0,1)，则这k个随机变量的平方和</p><script type="math/tex; mode=display">X = \sum_{i=1}^kZ_i^2</script><p>为服从自由度为k的卡方分布，记为：</p><script type="math/tex; mode=display">X ～ \chi^2(k) \quad或 X ～ \chi_k^2</script><p>卡方分布的期望与方差分为为：<br>$E(\chi^2)=n，D(\chi^2)=2n$，其中n为卡方分布的自由度。</p><h4 id="7-2-卡方检验"><a href="#7-2-卡方检验" class="headerlink" title="7.2 卡方检验"></a>7.2 卡方检验</h4><p>χ2检验是以χ2分布为基础的一种假设检验方法，主要用于分类变量。其基本思想是根据样本数据推断总体的分布与期望分布是否有显著性差异，或者推断两个分类变量是否相关或者独立。<br>一般可以设原假设为 H0：观察频数与期望频数没有差异，或者两个变量相互独立不相关。<br>实际应用中，我们先假设H0成立，计算出χ2的值，χ2表示观察值与理论值之间的偏离程度。根据χ2分布，χ2统计量以及自由度，可以确定在H0成立的情况下获得当前统计量以及更极端情况的概率p。如果p很小，说明观察值与理论值的偏离程度大，应该拒绝原假设。否则不能拒绝原假设。</p><p>χ2的计算公式为：</p><script type="math/tex; mode=display">\chi^2 = \sum \frac{(A-T)^2}{T}</script><p>其中，A为实际值，T为理论值。<br>χ2用于衡量实际值与理论值的差异程度，这也是卡方检验的核心思想。χ2包含了以下两个信息：<br>1.实际值与理论值偏差的绝对大小。<br>2.差异程度与理论值的相对大小。</p><h4 id="7-3-卡方检验做特征选择"><a href="#7-3-卡方检验做特征选择" class="headerlink" title="7.3 卡方检验做特征选择"></a>7.3 卡方检验做特征选择</h4><p>卡方检验经常被用来做特征选择。举个网络上的例子，假设我们有一堆新闻标题，需要判断标题中包含某个词（比如吴亦凡）是否与该条新闻的类别归属（比如娱乐）是否有关，我们只需要简单统计就可以获得这样的一个四格表：</p><div class="table-container"><table><thead><tr><th style="text-align:center">组别</th><th style="text-align:center">属于娱乐</th><th style="text-align:center">不属于娱乐</th><th style="text-align:center">合计</th></tr></thead><tbody><tr><td style="text-align:center">不包含吴亦凡</td><td style="text-align:center">19</td><td style="text-align:center">24</td><td style="text-align:center">43</td></tr><tr><td style="text-align:center">包含吴亦凡</td><td style="text-align:center">34</td><td style="text-align:center">10</td><td style="text-align:center">44</td></tr><tr><td style="text-align:center">合计</td><td style="text-align:center">53</td><td style="text-align:center">34</td><td style="text-align:center">87</td></tr></tbody></table></div><p>首先假设标题是否包含吴亦凡与新闻是否属于娱乐是独立无关的，随机抽取一条新闻标题，属于娱乐类别的概率是：(19 + 34) / (19 + 34 + 24 +10) = 60.9%<br>理论值的四格表为：</p><div class="table-container"><table><thead><tr><th style="text-align:center">组别</th><th style="text-align:center">属于娱乐</th><th style="text-align:center">不属于娱乐</th><th style="text-align:center">合计</th></tr></thead><tbody><tr><td style="text-align:center">不包含吴亦凡</td><td style="text-align:center">43*0.609=26.2</td><td style="text-align:center">43*0.391=16.8</td><td style="text-align:center">43</td></tr><tr><td style="text-align:center">包含吴亦凡</td><td style="text-align:center">44*0.609=26.8</td><td style="text-align:center">44*0.391=17.2</td><td style="text-align:center">44</td></tr></tbody></table></div><p>显然，如果两个变量是独立无关的，那么四格表中的理论值与实际值的差异会非常小。<br>则χ2值为:</p><script type="math/tex; mode=display">\chi^2 = \frac{(19-26.2)^2}{26.2}+\frac{(34-26.8)^2}{26.8}+\frac{(24-16.8)^2}{16.8}+\frac{(10-17.2)^2}{17.2}=10</script><p>标准的四格表χ2值可以用以下方式进行计算：</p><script type="math/tex; mode=display">\chi^2 = \frac{N*(AD-BC)^2}{(A+B)(C+D)(A+C)(B+D)}</script><p>其中，$N=A+B+C+D$<br>得到χ2的值以后，怎样可以得知无关性假设是否可靠？接下来我们应该查询卡方分布的临界值表了。<br>首先我们明确自由度的概念：$自由度v=(行数-1)\ast(列数-1)$。<br>然后看卡方分布的临界概率，表如下：<br><img src="/images/2018.3.04_3.jpg" alt=""><br>一般我们取p=0.05，也就是说两者不相关的概率为0.05时，对应的卡方值为3.84。显然10.0&gt;3.84，那就说明包含吴亦凡的新闻不属于娱乐的概率小于0.05。换句话说，包含吴亦凡的新闻与娱乐新闻相关的概率大于95%！<br>总结一下：我们可以通过卡方值来判断特征是否与类型有关。卡方值越大，说明关联越强，特征越需要保留。卡方值越小，说明越不相关，特征需要去除。</p><h3 id="8-HMM-Hidden-Markov-model-9"><a href="#8-HMM-Hidden-Markov-model-9" class="headerlink" title="8. HMM(Hidden Markov model) [9]"></a>8. HMM(Hidden Markov model) [9]</h3><p>HMM模型就是这样一个系统——它有一个会随时间改变的隐藏的状态，在持续地影响它的外在表现。  </p><p>假设我手里有三个不同的骰子。第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。<br><img src="/images/2018.3.04_4.jpg" alt=""><br>假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4  </p><p>这串数字叫做可见状态链。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。</p><p>同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。<br><img src="/images/2018.3.04_5.jpg" alt=""><br><img src="/images/2018.3.04_6.jpg" alt=""><br>其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。和HMM模型相关的算法主要分为三类，分别解决三种问题：</p><p>1）知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。<br>2）还是知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。<br>3）知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我想反推出每种骰子是什么（转换概率）。</p><p>HMM的五样“要素”：<br>1.状态和状态间转换的概率<br>2.不同状态下，有着不同的外在表现的概率。<br>3.最开始设置的初始状态<br>4.能转换的所有状态的集合<br>5.能观察到外在表现的结合<br>Hidden 说明的是状态的不可见性；Markov说明的是状态和状态间是markov chain。这就是为什么叫Hidden Markov Model。</p><h3 id="9-Cross-validation-10"><a href="#9-Cross-validation-10" class="headerlink" title="9.Cross-validation [10]"></a>9.Cross-validation [10]</h3><h4 id="9-1-训练机-VS-测试集"><a href="#9-1-训练机-VS-测试集" class="headerlink" title="9.1 训练机 VS 测试集"></a>9.1 训练机 VS 测试集</h4><p>在模式识别（pattern recognition）与机器学习（machine learning）的相关研究中，经常会将数据集（dataset）分为训练集（training set）跟测试集（testing set）这两个子集，前者用以建立模型（model），后者则用来评估该模型对未知样本进行预测时的精确度，正规的说法是泛化能力（generalization ability）。怎么将完整的数据集分为训练集跟测试集，必须遵守如下要点：</p><p>1、只有训练集才可以用在模型的训练过程中，测试集则必须在模型完成之后才被用来评估模型优劣的依据。<br>2、训练集中样本数量必须够多，一般至少大于总样本数的50%。<br>3、两组子集必须从完整集合中均匀取样。</p><h4 id="9-2-交叉验证（Cross-validation）"><a href="#9-2-交叉验证（Cross-validation）" class="headerlink" title="9.2 交叉验证（Cross-validation）"></a>9.2 交叉验证（Cross-validation）</h4><p> 交叉验证（Cross Validation）是用来验证分类器的性能一种统计分析方法，基本思想是把在某种意义下将原始数据（dataset）进行分组，一部分做为训练集（training set），另一部分做为验证集（validation set），首先用训练集对分类器进行训练，在利用验证集来测试训练得到的模型（model），以此来做为评价分类器的性能指标。常见的交叉验证方法如下：</p><h5 id="9-2-1-Hold-Out-Method"><a href="#9-2-1-Hold-Out-Method" class="headerlink" title="9.2.1 Hold-Out Method"></a>9.2.1 Hold-Out Method</h5><p>将原始数据随机分为两组，一组做为训练集，一组做为验证集，利用训练集训练分类器，然后利用验证集验证模型，记录最后的分类准确率为此分类器的性能指标。此种方法的好处的处理简单，只需随机把原始数据分为两组即可，其实严格意义来说Hold-Out Method并不能算是CV，因为这种方法没有达到交叉的思想，由于是随机的将原始数据分组，所以最后验证集分类准确率的高低与原始数据的分组有很大的关系，所以这种方法得到的结果其实并不具有说服性。</p><h5 id="9-2-2-Double-Cross-Validation（2-fold-Cross-Validation，记为2-CV）"><a href="#9-2-2-Double-Cross-Validation（2-fold-Cross-Validation，记为2-CV）" class="headerlink" title="9.2.2 Double Cross Validation（2-fold Cross Validation，记为2-CV）"></a>9.2.2 Double Cross Validation（2-fold Cross Validation，记为2-CV）</h5><p>做法是将数据集分成两个相等大小的子集，进行两回合的分类器训练。在第一回合中，一个子集作为training set，另一个便作为testing set；在第二回合中，则将training set与testing set对换后，再次训练分类器，而其中我们比较关心的是两次testing sets的辨识率。不过在实务上2-CV并不常用，主要原因是training set样本数太少，通常不足以代表母体样本的分布，导致testing阶段辨识率容易出现明显落差。此外，2-CV中分子集的变异度大，往往无法达到“实验过程必须可以被复制”的要求。</p><h5 id="9-2-3-K-fold-Cross-Validation（K-折交叉验证，记为K-CV）"><a href="#9-2-3-K-fold-Cross-Validation（K-折交叉验证，记为K-CV）" class="headerlink" title="9.2.3 K-fold Cross Validation（K-折交叉验证，记为K-CV）"></a>9.2.3 K-fold Cross Validation（K-折交叉验证，记为K-CV）</h5><p>将原始数据分成K组（一般是均分），将每个子集数据分别做一次验证集，其余的K-1组子集数据作为训练集，这样会得到K个模型，用这K个模型最终的验证集的分类准确率的平均数作为此K-CV下分类器的性能指标。K一般大于等于2，实际操作时一般从3开始取，只有在原始数据集合数据量小的时候才会尝试取2。K-CV可以有效的避免过学习以及欠学习状态的发生，最后得到的结果也比较具有说服性。一般取 k=10.</p><h5 id="9-2-4-Leave-One-Out-Cross-Validation（记为LOO-CV）"><a href="#9-2-4-Leave-One-Out-Cross-Validation（记为LOO-CV）" class="headerlink" title="9.2.4 Leave-One-Out Cross Validation（记为LOO-CV）"></a>9.2.4 Leave-One-Out Cross Validation（记为LOO-CV）</h5><p>如果设原始数据有N个样本，那么LOO-CV就是N-CV，即每个样本单独作为验证集，其余的N-1个样本作为训练集，所以LOO-CV会得到N个模型，用这N个模型最终的验证集的分类准确率的平均数作为此下LOO-CV分类器的性能指标。相比于前面的K-CV，LOO-CV有两个明显的优点：<br>（1）每一回合中几乎所有的样本皆用于训练模型，因此最接近原始样本的分布，这样评估所得的结果比较可靠。<br>（2）实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。<br>但LOO-CV的缺点则是计算成本高，因为需要建立的模型数量与原始数据样本数量相同，当原始数据样本数量相当多时，LOO-CV在实作上便有困难几乎就是不显示，除非每次训练分类器得到模型的速度很快，或是可以用并行化计算减少计算所需的时间。</p><h4 id="9-3-使用Cross-Validation时常犯的错误"><a href="#9-3-使用Cross-Validation时常犯的错误" class="headerlink" title="9.3 使用Cross-Validation时常犯的错误"></a>9.3 使用Cross-Validation时常犯的错误</h4><p>由于实验室许多研究都有用到 evolutionary algorithms（EA）与 classifiers，所使用的 fitness function 中通常都有用到 classifier 的辨识率，然而把cross-validation 用错的案例还不少。前面说过，只有 training data 才可以用于 model 的建构，所以只有 training data 的辨识率才可以用在 fitness function 中。而 EA 是训练过程用来调整 model 最佳参数的方法，所以只有在 EA结束演化后，model 参数已经固定了，这时候才可以使用 test data。那 EA 跟 cross-validation 要如何搭配呢？Cross-validation 的本质是用来估测(estimate)某个 classification method 对一组 dataset 的 generalization error，不是用来设计 classifier 的方法，所以 cross-validation 不能用在 EA的 fitness function 中，因为与 fitness function 有关的样本都属于 training set，那试问哪些样本才是 test set 呢？如果某个 fitness function 中用了cross-validation 的 training 或 test 辨识率，那么这样的实验方法已经不能称为 cross-validation 了。</p><p>EA 与 k-CV 正确的搭配方法，是将 dataset 分成 k 等份的 subsets 后，每次取 1份 subset 作为 test set，其余 k-1 份作为 training set，并且将该组 training set 套用到 EA 的 fitness function 计算中(至于该 training set 如何进一步利用则没有限制)。因此，正确的 k-CV 会进行共 k 次的 EA 演化，建立 k 个classifiers。而 k-CV 的 test 辨识率，则是 k 组 test sets 对应到 EA 训练所得的 k 个 classifiers 辨识率之平均值。</p><h3 id="10-混合线性模型（Linear-mixed-models，LMM"><a href="#10-混合线性模型（Linear-mixed-models，LMM" class="headerlink" title="10. 混合线性模型（Linear mixed models，LMM)"></a>10. 混合线性模型（Linear mixed models，LMM)</h3><p><a href="http://www.cnblogs.com/leezx/p/7429768.html" target="_blank" rel="noopener">http://www.cnblogs.com/leezx/p/7429768.html</a></p><h3 id="11-逻辑回归"><a href="#11-逻辑回归" class="headerlink" title="11. 逻辑回归"></a>11. 逻辑回归</h3><p><a href="https://www.cnblogs.com/sparkwen/p/3441197.html" target="_blank" rel="noopener">https://www.cnblogs.com/sparkwen/p/3441197.html</a></p><h3 id="12-降维算法"><a href="#12-降维算法" class="headerlink" title="12. 降维算法"></a>12. 降维算法</h3><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1.<a href="http://blog.csdn.net/adminabcd/article/details/51429561" target="_blank" rel="noopener">http://blog.csdn.net/adminabcd/article/details/51429561</a><br>2.<a href="http://www.cnblogs.com/sxron/p/5451923.html" target="_blank" rel="noopener">http://www.cnblogs.com/sxron/p/5451923.html</a><br>3.<a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html" target="_blank" rel="noopener">http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html</a><br>4.<a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="noopener">http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</a><br>5.<a href="http://cpmarkchang.logdown.com/posts/193261-machine-learning-overfitting-and-regularization" target="_blank" rel="noopener">http://cpmarkchang.logdown.com/posts/193261-machine-learning-overfitting-and-regularization</a><br>6.<a href="https://www.zhihu.com/question/21094489" target="_blank" rel="noopener">https://www.zhihu.com/question/21094489</a><br>7.<a href="https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html" target="_blank" rel="noopener">https://wizardforcel.gitbooks.io/dm-algo-top10/content/apriori.html</a><br>8.<a href="http://blog.csdn.net/bitcarmanlee/article/details/52279907" target="_blank" rel="noopener">http://blog.csdn.net/bitcarmanlee/article/details/52279907</a><br>9.<a href="https://www.zhihu.com/question/20962240" target="_blank" rel="noopener">https://www.zhihu.com/question/20962240</a><br>10.<a href="http://blog.csdn.net/holybin/article/details/27185659" target="_blank" rel="noopener">http://blog.csdn.net/holybin/article/details/27185659</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此篇主要记录自己在学习数据挖掘过程中遇到的一些基本算法，目的是做一个整理，方便自己随时查阅。由于这些算法已经比较普及了，所以为了节省时间，大多参考网上前人的解释。&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="http://memicat.com/categories/Data-Mining/"/>
    
    
      <category term="K-means" scheme="http://memicat.com/tags/K-means/"/>
    
      <category term="KNN" scheme="http://memicat.com/tags/KNN/"/>
    
      <category term="Naıve Bayes）" scheme="http://memicat.com/tags/Naive-Bayes%EF%BC%89/"/>
    
      <category term="Decision tree" scheme="http://memicat.com/tags/Decision-tree/"/>
    
      <category term="SVM" scheme="http://memicat.com/tags/SVM/"/>
    
      <category term="Apriori" scheme="http://memicat.com/tags/Apriori/"/>
    
      <category term="chi-square distribution" scheme="http://memicat.com/tags/chi-square-distribution/"/>
    
      <category term="HMM(Hidden Markov model)" scheme="http://memicat.com/tags/HMM-Hidden-Markov-model/"/>
    
      <category term="Cross-validation" scheme="http://memicat.com/tags/Cross-validation/"/>
    
  </entry>
  
  <entry>
    <title>基于模式的序列分类算法</title>
    <link href="http://memicat.com/2018/03/01/2018.03.01_%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%BA%8F%E5%88%97%E5%88%86%E7%B1%BB/"/>
    <id>http://memicat.com/2018/03/01/2018.03.01_基于模式的序列分类/</id>
    <published>2018-02-28T23:00:00.000Z</published>
    <updated>2018-03-06T17:32:23.221Z</updated>
    
    <content type="html"><![CDATA[<p>Sequence classification is an important task in data mining. We address the problem of sequence classification using rules composed of interesting patterns found in a dataset of labelled sequences and accompanying class labels. We use the discovered patterns to generate confident classification rules, and present two different ways of building a classifier. The patterns considered in this paper could be both itemsets or subsequences (sequential patterns).<br><a id="more"></a></p><h3 id="1-Problem-Statement"><a href="#1-Problem-Statement" class="headerlink" title="1. Problem Statement"></a>1. Problem Statement</h3><p>Define an event e as a $pair(i,t)$, consisting of an item $i \in I$ and a time stamp $t \in N$, where $I$ is the set of all possible items and N is the set of natural numbers. Denote a sequence of such events by $s=\langle e_1,e_2,…,e_l\rangle$, $l$ is the length of sequence. A set $X = \{ i_1,i_2,…,i_n\} $is called an itemset.<br>Let $L$ be a finite set of class labels and $|L|$ the number of classes.<br>Example 1: Given a training dataset of two classes as shown in Table 1. Then, $S = \{s_1,s_2,…,s_8\} $and $S_1=\{s_1,s_2,s_3,s_4\}$.<br><img src="/images/2018.3.01_1.png" alt=""></p><h4 id="1-1-Definition-of-an-Interesting-pattern"><a href="#1-1-Definition-of-an-Interesting-pattern" class="headerlink" title="1.1 Definition of an Interesting pattern"></a>1.1 Definition of an Interesting pattern</h4><p>The interestingness of a pattern depends on two factors: its support and its cohesion.  </p><h5 id="1-1-1-Support"><a href="#1-1-1-Support" class="headerlink" title="1.1.1 Support"></a>1.1.1 Support</h5><p>The support of a pattern $P$ in a given class of sequences $S_k$ can now be defined as $F_k(P)=\frac{|N_k(P)|}{|S_k|}$, where P could be $X$ or $s^\prime$.<br>Example 2. Consider the dataset shown in Table 1. Given $X=\{a,b\}$, we see that $F_1(X)=4/4=1$. If$s^\prime= \langle a,b\rangle$,then $F_1(s^\prime) = 3/4 =0.75$</p><h5 id="1-1-2-Cohesion"><a href="#1-1-2-Cohesion" class="headerlink" title="1.1.2 Cohesion"></a>1.1.2 Cohesion</h5><p>How close the items are to each other on average, using the lengths of the shortest intervals containing the pattern in different sequences.<br>Defining the length of the shortest interval containing an itemset X in a sequence $s\in N(X)$ as <script type="math/tex">W(X,s)=min\lbrace  t_2-t_1+1|t_1 \le t_2 \quad and \quad  \forall i \in X,\exists (i,t)\in s, where \quad t_1 \le t \le t_2 \rbrace</script>.<br>In order to compute the cohesion of a pattern P within class k, we now compute the average length of such shortest intervals in <script type="math/tex">N_k(P):\bar {\mkern1mu W_k}(P)=\frac{\sum_{s \in N_k(P)}W(P,s)}{|N_k(P)|}</script>, where P could be X or <script type="math/tex">s^\prime</script>.<br>It is clear that $\bar {\mkern1mu W_k}(P)$is greater than or equal to the number of items in P, denoted as $|P|$. Furthermore, for a fully cohesive pattern(All partens containing just one item), $\bar {\mkern1mu W_k}(P) = |P|$. Therefore, we define cohesion pf P in $N_k(P)$ as $C_k(P)=\frac{|p|}{\bar {\mkern1mu W_k}(P)}$.<br>The cohesion of P in a single sequence s is defined as $C_k(P,s)=\frac{|p|}{\bar {\mkern1mu W_k}(P,s)}$.<br>Example 3. Consider again the dataset given in Table 1,assume $X=\{b,d\}$,then $C_2(X) = \frac{2}{(2+4+2+2)/4}=0.8$ and $C(X,s_3)=2/2=1$.</p><h5 id="1-1-3-Interestingness"><a href="#1-1-3-Interestingness" class="headerlink" title="1.1.3 Interestingness"></a>1.1.3 Interestingness</h5><p>In a given class of sequences <script type="math/tex">S_k</script>, we can now define the interestingness of a pattern P as <script type="math/tex">I_k(P)=F_k(P) \times C_k(P)</script>, where P could be X or <script type="math/tex">s^\prime</script>.<br>Given a minimum support threshold min_sup and a minimum interestingness threshold min_int, a pattern P is considered interesting in a set of sequences labelled by class label <script type="math/tex">Lk</script>, if <script type="math/tex">F_k(P) \ge min_s</script> and <script type="math/tex">I_k(P) \ge min_i</script>.</p><h4 id="1-2-Classification-Rules"><a href="#1-2-Classification-Rules" class="headerlink" title="1.2 Classification Rules"></a>1.2 Classification Rules</h4><p>Define $r : P \Rightarrow: L_k$ as a classification rule. The confidence of a rule can now be defined as</p><script type="math/tex; mode=display">conf(P \Rightarrow L_k) = \frac{|N_k(P)|}{|N(P)|}</script><p>A rule $P \Rightarrow L_k$ is considered confident if its confidence exceeds a given threshold min_conf.</p><h3 id="2-Rule-based-classifiers"><a href="#2-Rule-based-classifiers" class="headerlink" title="2. Rule based classifiers"></a>2. Rule based classifiers</h3><p>Our algorithm, sequence classification based on interesting patterns, consists of two stages:</p><ol><li>Rule generation (called SCIP-RG, with one variant using interesting itemsets (SCII- RG),another using interesting subsequences (SCIS- RG)).</li><li>Classifier building (called SCIP-CB).</li></ol><h4 id="2-1-Generating-Interesting-Itemsets"><a href="#2-1-Generating-Interesting-Itemsets" class="headerlink" title="2.1 Generating Interesting Itemsets"></a>2.1 Generating Interesting Itemsets</h4><p><img src="/images/2018.3.01_2.png" alt=""><br>Algorithm 1: Line 1 counts the supports of all the items to determine the frequent items. Line 2 stores the interesting items in T1 (note that the interestingness of a single item is equal to its support). Lines 3-10 discover all interesting itemsets of different sizes n $(max-size \ge n \ge 2)$. First, the already discovered frequent itemsets of size n-1 $(A_{n-1})$ are used to generate the candidate itemsets $C_n$ using the candidateGen function (line 5). The candidateGen function is similar to the function Apriori-gen in the Apriori algorithm. In line 6, we store the frequent itemsets from $C_n$ into $A_n$. Line 7 stores the interesting itemsets from $A_n$ into $T_n$. The final set of all interesting itemsets in $S_k$ is stored in $\chi_k$ and produced as output.<br><img src="/images/2018.3.01_3.png" alt=""><br>Algorithm 2: To find the shortest interval in the entire sequence, it is enough to select a single item in X and to identify the shortest intervals for each time stamp at which this item occurs. To make the loop process faster, we pick the item in X that has the lowest frequency in s. We keep the set of candidate intervals associated with X in a list CW (line 1). To find the candidate interval around position $t_j$ containing all items of X, we start by looking for the nearest occurrences of items of X both left and right of position $t_j$ (lines 4-8). Note that if an item ak does not occur at all before the current time stamp, we set its left time stamp $l_j$ to $-\infty$, and if it does not occur after $t_j$, we set its right time stamp $r_j$ to $\infty$.</p><p><img src="/images/2018.3.01_4.png" alt=""><br>Algorithm 3: We start off with $l_j$ set to the smallest time stamp in $L_j$ and $r_j$ set to $t_j$, which represents the smallest occurrence of X such that all items in X occur at time stamps smaller than or equal to $t_j$. The process continues by moving to the next starting point $l_j$ (lines 5-6) and gets a new ending point $r_j$ corresponding to the item of the last starting point (lines 7- 8). If the new interval is shorter than the shortest interval found so far, we update the shortest interval length (line 10). Once again, if we have found an interval of length $|X|$ or the interval on the other side has grown sufficiently to make it impossible to get a smaller shortest interval, we stop the procedure and return $w$ (line 11). Otherwise, the procedure stops when we have processed all the time stamps in $L_j$ .<br>Example 4. Assume $s=\langle a,b,c,d,a,c\rangle$ and we want to find the shortest interval of $X=\{a,b,c,d\}$. First, we pick the the least frequent item b and thus $t_j = 2$. We obtain $L_j=[-\infty,-\infty,1]$ and $R_j= [3,4,5]$ for items c, d and a. We then get the first candidate interval of length $\infty$ based on lines 1-3 of Algorithm 3. Executing the for loop of Algorithm 3, we then get a new candidate interval of length $\infty$ with $l_j=-\infty, r_j=3$ when $k=2$ and finally get another candidate interval of length 4 with $l_j=1,r_j=4$ when $k=3$. Therefore, we stop the procedure and return $W(X,s)=4$.</p><h4 id="2-2-Generating-Interesting-Subsequences"><a href="#2-2-Generating-Interesting-Subsequences" class="headerlink" title="2.2 Generating Interesting Subsequences"></a>2.2 Generating Interesting Subsequences</h4><p>In Algorithm 4, lines 1-2 store the interesting 1-sequences in T1. Line 4 calls Algorithm 5 to get interesting n-sequences $(2 \le n \le max-size)$. Finally, we get the complete set of interesting subsequences Yk (line 5).<br><img src="/images/2018.3.01_5.png" alt=""><br>In Algorithm 5, given any two l-sequences $\alpha_i$ and $\alpha_j$ that share the same (l-1) length prefix, we generate a candidate sequence $s^\prime$ of length (l+1) by adding the last item in $\alpha_j$ to $\alpha_i$ (line 4). In order to determine the cohesion of a subsequence $s^\prime$, needed to compute $I_k(s^\prime)$ in line 7, we had to make a modification to the original SPADE algorithm. We added a function that finds the shortest intervals of a frequent subsequence $s^\prime$ in an input sequence s by tracking the occurrences in s of all the items contained in $s^\prime$.<br><img src="/images/2018.3.01_6.png" alt=""></p><h4 id="2-3-Pruning-the-Rules"><a href="#2-3-Pruning-the-Rules" class="headerlink" title="2.3 Pruning the Rules"></a>2.3 Pruning the Rules</h4><p>Definition 1. Given two rules in $R,r_i$ and $r_j$, $r_i \succ r_j$ (also called $r_i$ precedes $r_j$ or $r_i$ has a higher precedence than $r_j$) if:</p><ol><li>the confidence of $r_i$ is greater than that of $r_j$, or</li><li>the confidence of $r_i$ and $r_j$ is the same, but the interestingness of $r_i$  is greater than that of $r_j$ , or</li><li>both the confidence and the interestingness of $r_i$ and $r_j$ are the same, but the size of $r_i$ is greater than that of $r_j$.</li><li>all of the three parameters are the same, but $r_i$ is generated earlier than $r_j$.<br><img src="/images/2018.3.01_7.png" alt=""><br>The algorithm 6, has two main steps. First, we sort the set of confident rules R according to Definition 1 (line 1). This makes it faster to get good rules for classifying. Then, in lines 2-10, we prune the rules using the database coverage method. For each rule r in sorted R, we go through the data-set D to find all the data objects correctly classified by r and increase the cover counts of those data objects (lines 4-8). We store the rule r into PR if it correctly classifies a data object (line 7). If the cover count of a data object passes the coverage threshold, we remove the data object (line 10). Line 11 returns the new set of rules PR. In the worst case, to check whether a data object is correctly classified by r, we might need to read the whole sequence part s of the data object, resulting in a time complexity of OðjsjÞ.<br><img src="/images/2018.3.01_8.png" alt=""></li></ol><h4 id="2-4-Building-the-Classifiers"><a href="#2-4-Building-the-Classifiers" class="headerlink" title="2.4 Building the Classifiers"></a>2.4 Building the Classifiers</h4><p>When classifying a new data object, HARMONY computes the score of a class label $L_k$ as the sum of the top $\lambda$ highest confidences of the rules carrying class label $L_k$ and matching the data object.</p><ol><li>SCIP_HAR (HARMONY based classifier): allow different support thresholds for different classes.</li><li>SCIP_MA, Matching cohesive rules based classifier.<br>HARMONY does not account for the possibility that there may not exist a rule matching the given data object which decreases its accuracy performance. We fix this by adding a default rule,$null \Rightarrow L_d$, to the classifier.<br><img src="/images/2018.3.01_9.png" alt=""><br>First, we find all the rules that match the given data object d and store them into MR (lines 1-3). Then, we handle two different cases:</li><li>(lines 4-15): If the size of MR is greater than 0, we compute the $r.value$ of every rule in MR and sort the rules according to $r.value$ (the higher the value, the higher the precedence). Lines 6-9 compute the score of a rule for SCIP_HAR and SCIP_MA, respectively. We finally utilise the top $\lambda$ rules in the sorted MR, denoted by CR, to classify the given data object. If the number of rules in MR is smaller than $\lambda$, we simply use all the rules in MR as CR. Lines 12-14 compute the sum of $r.values$ of each rule in CR according to their class labels and line 15 returns the class label which has the largest sum.</li><li>(line 16): If MR is empty, we return the class label of the default rule.<br><img src="/images/2018.3.01_10.png" alt=""></li></ol><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>Zhou C, Cule B, Goethals B. Pattern based sequence classification[J]. IEEE Transactions on Knowledge and Data Engineering, 2016, 28(5): 1285-1298.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sequence classification is an important task in data mining. We address the problem of sequence classification using rules composed of interesting patterns found in a dataset of labelled sequences and accompanying class labels. We use the discovered patterns to generate confident classification rules, and present two different ways of building a classifier. The patterns considered in this paper could be both itemsets or subsequences (sequential patterns).&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="http://memicat.com/categories/Data-Mining/"/>
    
    
      <category term="Sequence classification" scheme="http://memicat.com/tags/Sequence-classification/"/>
    
      <category term="Interesting patterns" scheme="http://memicat.com/tags/Interesting-patterns/"/>
    
      <category term="Classification rules" scheme="http://memicat.com/tags/Classification-rules/"/>
    
      <category term="Feature vectors" scheme="http://memicat.com/tags/Feature-vectors/"/>
    
  </entry>
  
  <entry>
    <title>一些零散的Machine learning 知识</title>
    <link href="http://memicat.com/2018/02/28/2018.02.28_%E4%B8%80%E4%BA%9B%E9%9B%B6%E6%95%A3%E7%9A%84Machine%20learning%20%E7%9F%A5%E8%AF%86/"/>
    <id>http://memicat.com/2018/02/28/2018.02.28_一些零散的Machine learning 知识/</id>
    <published>2018-02-27T23:00:00.000Z</published>
    <updated>2018-03-06T17:32:25.034Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Basic-knowledge"><a href="#1-Basic-knowledge" class="headerlink" title="1. Basic knowledge"></a>1. Basic knowledge</h3><h4 id="1-1-What-is-an-F-Statistic"><a href="#1-1-What-is-an-F-Statistic" class="headerlink" title="1.1 What is an F Statistic?"></a>1.1 What is an F Statistic?</h4><p>An F statistic is a value you get when you run an ANOVA test or a regression analysis to find out if the means between two populations are significantly different. It’s similar to a T statistic from a T Test; A T test will tell you if a single variable is statistically significant and an F test will tell you if a group of variables are jointly significant.<br><a id="more"></a><br><img src="/images/2018.2.28_1.jpg" alt="" title="Fig. 1. F Statistic"><br>F value and an F critical value:<br>(1)The F critical value is also called the F statistic.<br>(2)The value you calculate from your data is called the F value (without the “critical” part).<br>In general, if your calculated F value in a test is larger than your F statistic, you can reject the null hypothesis.</p><h4 id="1-2-样本方差为什么除以”n-1“"><a href="#1-2-样本方差为什么除以”n-1“" class="headerlink" title="1.2 样本方差为什么除以”n-1“?"></a>1.2 样本方差为什么除以”n-1“?</h4><p>一般遇到求样本方差时，会有下面两个式子：</p><script type="math/tex; mode=display">\bar {\mkern1mu X} = \frac{1}{n} \sum_{i=1}^n X_i, \quad \sigma^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar {\mkern1mu X})^2</script><p>很多人可能会疑惑，为什么方差的求解式里面是除以“n-1”而不是“n”。 事实上，这个“n-1”不是我们直观上的总样本个数，而是样本变量自由度。用自由度求出的方差称为无偏方差（无偏估计），可以用来估计样本的总体方差。<br>我们求样本方差时往往已经进行过对样本均值的求解，也就是说先计算样本均值，然后计算样本方差。在计算完样本均值以后，样本内就只有 n-1 个样本的值是可以变化的。<br>假设有3个样本，$X_1,X_2,X_3$.通常来说，这三个变量的取值都是不定的。但是假如我们已经知道这三个样本的平均值为5，那么具有“随机性”的样本就只有2个。因为只要随机取其两个样本的值，假设$X_1,X_2$，则第三个样本的值$X_3 = 15- X_1-X_2$, 所以自由度为2。而同样对于 n 个样本，留出一个自由度给固定的均值，剩下的自由度即为n-1.也可以理解为n个样本中的一个样本的值为 $\bar {\mkern1mu X} $，其他 n-1 个的样本均值也为 $\bar {\mkern1mu X} $，所以在求方差时，有一项 $(X_i - \bar {\mkern1mu X})^2$ 为0，所以忽略，故最终除以n-1.</p><h4 id="1-3-如何区分“Factor”-“Covariates”"><a href="#1-3-如何区分“Factor”-“Covariates”" class="headerlink" title="1.3 如何区分“Factor”, “Covariates”?"></a>1.3 如何区分“Factor”, “Covariates”?</h4><p>首先来区分Factor（因子） &amp; Covariates （协变量）：<br>前者是名目变量，一般只含两至数个类别，每个类别至少有30个案例，后者是连续或定距变量（可以含成千上百个类别，每个类别中只含一至数个案例）。协变量一般用来指“控制变量”，可以说连续变量（如年龄）、也可以是名目变量（如性别）。<br>Fixed factor: qualitative covariate，固定因子 = 定性协变量， 例如gender, agegroup.<br>Fixed effect: quantitative covariate，定量协变量， 例如age.<br>Random factor: qualitative variable whose levels are randomly sampled from a population of levels being studied.<br>Random effect：quantitative variable whose levels are randomly sampled from a population of levels being studied.</p><h4 id="1-4-指数加权移动平均值-Exponentially-Weighted-Moving-Average-EWMA"><a href="#1-4-指数加权移动平均值-Exponentially-Weighted-Moving-Average-EWMA" class="headerlink" title="1.4 指数加权移动平均值(Exponentially Weighted Moving-Average,EWMA)"></a>1.4 指数加权移动平均值(Exponentially Weighted Moving-Average,EWMA)</h4><p>在时间 t, 根据实际的观测值（或量测值）我们可以求取 EWMA（t）如下：<br>$EWMA(t ) = λY(t)+ ( 1-λ) EWMA(t-1)$ for t = 1, 2, …, n.</p><ul><li>EWMA（t）：t时刻的估计值</li><li>Y（t）： t 时间之量测值</li><li>n is the number of observations to be monitored including EWMA0</li><li>λ ( 0 &lt; λ&lt; 1 ) ﹐表EWMA对于历史量测值之权重系数﹐其值越接近1，表对过去量测值的权重较低。</li></ul><p>从另一个角度看， λ 决定了EWMA估计器跟踪实际数据突然发生变化的能力，即时效性， 显然随着λ 增大， 估计器的时效性就越强，反之，越弱; 另一方面，由于 λ 的存在，EWMA还表现出一定的吸收瞬时突发的能力，这种能力称为平稳性。显然随着 λ 减小， 估计器的平稳性增强，反之降低。</p><p>1.从概率角度看，EWMA是一种理想的最大似然估计技术，它采用一个权重因子 λ 对数据进行估计，当前估计值由前一次估计值和当前的抽样值共同决定。<br>2.从信号处理角度看，EWMA可以看成是一个低通滤波器，通过控制 λ 值，剔除短期波动、保留长期发展趋势提供了信号的平滑形式。</p><h4 id="1-5-参数（Parameters）和超参数（Hyperparameters）-5"><a href="#1-5-参数（Parameters）和超参数（Hyperparameters）-5" class="headerlink" title="1.5 参数（Parameters）和超参数（Hyperparameters）[5]"></a>1.5 参数（Parameters）和超参数（Hyperparameters）[5]</h4><p>参数：就是模型可以根据数据可以自动学习出的变量，应该就是参数。比如，深度学习的权重，偏差等<br>超参数：就是用来确定模型的一些参数，超参数不同，模型是不同的(这个模型不同的意思就是有微小的区别，比如假设都是CNN模型，如果层数不同，模型不一样，虽然都是CNN模型)，超参数一般就是根据经验确定的变量。在深度学习中，超参数有：学习速率，迭代次数，层数，每层神经元的个数等等。</p><h4 id="1-6-多元正态分布"><a href="#1-6-多元正态分布" class="headerlink" title="1.6 多元正态分布"></a>1.6 多元正态分布</h4><p><a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83</a></p><h4 id="1-7-马氏距离"><a href="#1-7-马氏距离" class="headerlink" title="1.7 马氏距离"></a>1.7 马氏距离</h4><p><a href="https://baike.baidu.com/item/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB?fr=aladdin</a></p><h4 id="1-8-Jeffrey’s-prior"><a href="#1-8-Jeffrey’s-prior" class="headerlink" title="1.8 Jeffrey’s prior"></a>1.8 Jeffrey’s prior</h4><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1.<a href="https://www.zhihu.com/question/20983193" target="_blank" rel="noopener">https://www.zhihu.com/question/20983193</a><br>2.Cao S, Rhinehart R R. An efficient method for on-line identification of steady state[J]. Journal of Process Control, 1995, 5(6): 363-374.<br>3.<a href="http://zjz06.blogspot.se/2011/08/fixed-factorsrandom-factorscovariates.html" target="_blank" rel="noopener">http://zjz06.blogspot.se/2011/08/fixed-factorsrandom-factorscovariates.html</a><br>4.<a href="http://blog.csdn.net/kuvinxu/article/details/6922138" target="_blank" rel="noopener">http://blog.csdn.net/kuvinxu/article/details/6922138</a><br>5.<a href="http://blog.csdn.net/uestc_c2_403/article/details/77428736" target="_blank" rel="noopener">http://blog.csdn.net/uestc_c2_403/article/details/77428736</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-Basic-knowledge&quot;&gt;&lt;a href=&quot;#1-Basic-knowledge&quot; class=&quot;headerlink&quot; title=&quot;1. Basic knowledge&quot;&gt;&lt;/a&gt;1. Basic knowledge&lt;/h3&gt;&lt;h4 id=&quot;1-1-What-is-an-F-Statistic&quot;&gt;&lt;a href=&quot;#1-1-What-is-an-F-Statistic&quot; class=&quot;headerlink&quot; title=&quot;1.1 What is an F Statistic?&quot;&gt;&lt;/a&gt;1.1 What is an F Statistic?&lt;/h4&gt;&lt;p&gt;An F statistic is a value you get when you run an ANOVA test or a regression analysis to find out if the means between two populations are significantly different. It’s similar to a T statistic from a T Test; A T test will tell you if a single variable is statistically significant and an F test will tell you if a group of variables are jointly significant.&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="http://memicat.com/categories/Data-Mining/"/>
    
    
      <category term="Machine learning" scheme="http://memicat.com/tags/Machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>最小二乘估计（Least Squares Estimator）</title>
    <link href="http://memicat.com/2018/02/21/2018.02.21_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E4%BC%B0%E8%AE%A1/"/>
    <id>http://memicat.com/2018/02/21/2018.02.21_最小二乘估计/</id>
    <published>2018-02-20T23:00:00.000Z</published>
    <updated>2018-03-04T19:56:46.685Z</updated>
    
    <content type="html"><![CDATA[<p>最小二乘估计在Machine learning领域被广泛使用，接下来就来简单的介绍一下这个方法，首先，看一个式子：</p><script type="math/tex; mode=display">y=ax+e</script><p>这是一个简单的直线方程，X是自变量，y是因变量，<script type="math/tex">e</script>是一个常量，此等式只能代表一个自变量对因变量的影响。<br><a id="more"></a><br>如果我们把一个自变量拓展到n个自变量，比如影响一个人的智商的可能是年龄，但也有可能是环境，基因，受教育程度等。则以上式子就变为：</p><script type="math/tex; mode=display">y_i = a_1x_{i1} +...+ a_2x_{i2}+...+a_kx_{ik}+e_i, 1 \le i \le n, k\ge1</script><p>换做矩阵表示为：</p><script type="math/tex; mode=display">\vec{\mkern1mu y} = X \vec{\mkern1mu a} + \vec{\mkern1mu e}</script><script type="math/tex; mode=display">\left[      \begin{array}{c}        y1\\        y2\\        \vdots\\        y_n\\      \end{array}  \right]=    \left[  \begin{array}{cccc}  x_{11} & x_{12} & \cdots x_{1k} \\  x_{21} & x_{22} & \cdots x_{2k} \\  \vdots & \vdots & \ddots \vdots \\  x_{n1} & x_{n2} & \cdots x_{nk} \\  \end{array}  \right]  \left[  \begin{array}{c}    a1\\    a2\\    \vdots\\    a_k\\  \end{array}  \right] +\left[ \begin{array}{c}   e1\\   e2\\   \vdots\\   e_n\\ \end{array} \right]</script><p>为了让$\vec{\mkern1mu a}$变得更有意义，那么就要使$\vec{\mkern1mu e}$尽量的小。也就是让$\vec{\mkern1mu e}$的长度尽量小。</p><script type="math/tex; mode=display">|\vec{\mkern1mu e}| = \sqrt{\sum_{i=1}^n e_i^2}</script><p>两边平方得：</p><script type="math/tex; mode=display">|\vec{\mkern1mu e}|^2 = \sum_{i=1}^n e_i^2 = \vec{\mkern1mu e}\vec{\mkern1mu e} = \vec{\mkern1mu e}^{T}\vec{\mkern1mu e}</script><p>也就是说，当$\vec{\mkern1mu e}^{T}\vec{\mkern1mu e}$取最小值时，$\vec{\mkern1mu a}$能取得最优解。继续推倒，可以得到下式（证明略，有兴趣的可以看reference的第二个链接）：</p><script type="math/tex; mode=display">\vec{\mkern1mu e}^{T}\vec{\mkern1mu e} = \vec{\mkern1mu y}^{T}\vec{\mkern1mu y} - 2\vec{\mkern1mu y}^{T}X\vec{\mkern1mu a}+\vec{\mkern1mu a}^{T}X^{T}X\vec{\mkern1mu a}</script><p>然后利用矩阵微分，可得：</p><script type="math/tex; mode=display">\vec{\mkern1mu a} = (X^TX)^{-1}X^T\vec{\mkern1mu y}</script><p>这个式子就是最小二乘估计。</p><p>特殊情况下的最小二乘估计：如果X是一个方阵，则</p><script type="math/tex; mode=display">\vec{\mkern1mu a} = X^{-1}\vec{\mkern1mu y}</script><p>可以发现，当为方阵时，最小二乘估计很简单。不是方阵时，由于$X^{-1}$和$（X^T）^{-1}$都不成立，导致最小二乘估计无法继续化简。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="http://www.qiujiawei.com/linear-algebra-15/" target="_blank" rel="noopener">http://www.qiujiawei.com/linear-algebra-15/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最小二乘估计在Machine learning领域被广泛使用，接下来就来简单的介绍一下这个方法，首先，看一个式子：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=ax+e&lt;/script&gt;&lt;p&gt;这是一个简单的直线方程，X是自变量，y是因变量，&lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt;是一个常量，此等式只能代表一个自变量对因变量的影响。&lt;br&gt;
    
    </summary>
    
      <category term="Math" scheme="http://memicat.com/categories/Math/"/>
    
    
      <category term="Math learning" scheme="http://memicat.com/tags/Math-learning/"/>
    
      <category term="机器学习" scheme="http://memicat.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Least Squares Estimator" scheme="http://memicat.com/tags/Least-Squares-Estimator/"/>
    
      <category term="最小二乘估计" scheme="http://memicat.com/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>一个分布式挖掘快速在线学习算法 (Online learning algorithm)</title>
    <link href="http://memicat.com/2018/02/21/2018.02.21_%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%8C%96%E6%8E%98%E5%BF%AB%E9%80%9F%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    <id>http://memicat.com/2018/02/21/2018.02.21_一个分布式挖掘快速在线学习算法/</id>
    <published>2018-02-20T23:00:00.000Z</published>
    <updated>2018-03-04T19:56:39.345Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据异质性"><a href="#数据异质性" class="headerlink" title="数据异质性"></a>数据异质性</h3><p>假设自变量是X，因变量是Y。那么如果要识别X是如何影响Y的，则要保证其他因素（非观测因素）不变。为了逻辑清晰可见，把非观测因素的作用归结为Z。如果X的值变化了，Z的值也随之变动，此时你观测到的Y的变动，到底是X引起的，还是Z引起的？<br>如果其他因素Z不变，则其可以被看作是误差项，并认为这个误差项的方差应该是保持不变的，这样的就叫“同方差”。但是如果Z随X的变化而变化，那么Z的方差就不是不变的，也就是说误差项Z表现出“异方差”特性，也就是数据的”数据异质性“。<br><a id="more"></a></p><h3 id="欧几里得范数-（Euclidean-norm）"><a href="#欧几里得范数-（Euclidean-norm）" class="headerlink" title="欧几里得范数 （Euclidean norm）"></a>欧几里得范数 （Euclidean norm）</h3><p>也叫欧几里得长度，例如x是n维向量$(X_1,X_2,…,X_n)$</p><script type="math/tex; mode=display">||x|| = {||x||}_2 = \sqrt{|X_1|^2+|X_2|^2+...+|X_n|^2}</script><script type="math/tex; mode=display">{||x||}_2^2 = |X_1|^2+|X_2|^2+...+|X_n|^2</script><script type="math/tex; mode=display">{||x||}_1 = |X_1|+|X_2|+...+|X_n|</script><h3 id="协方差矩阵-（covariance-matrix）"><a href="#协方差矩阵-（covariance-matrix）" class="headerlink" title="协方差矩阵 （covariance matrix）"></a>协方差矩阵 （covariance matrix）</h3><p>方差是一种特殊的期望，被定义为：$Var(x)=E((x−E(x))^2)=E(x^2)-(E(x))^2$<br>在统计学与概率论中，协方差矩阵的每个元素是各个向量元素之间的协方差，是从标量随机变量到高维度随机向量的自然推广。协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。<br>协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。<br>期望值分别为E[X]与E[Y]的两个实随机变量X与Y之间的协方差Cov(X,Y)定义为：</p><script type="math/tex; mode=display">Cov(X,Y)    = E[(X-E[X])(Y-E[Y])]\\\qquad \qquad \qquad= E[XY] - 2E[Y]E[X]+E[X]E[Y]\\        \quad  = E[XY] - E[X]E[Y]</script><p>从直观上来看，协方差表示的是两个变量总体误差的期望。<br>如果X与Y是统计独立的，那么二者之间的协方差就是0，因为两个独立的随机变量满足<script type="math/tex">E[XY]=E[X]E[Y]</script>。<br>但是，反过来并不成立。即如果X与Y的协方差为0，二者并不一定是统计独立的。<br>协方差矩阵具有以下性质：<br>(1)  $cov(X,Y) = cov (Y,X)^T$<br>(2)  $cov(AX+b,Y) = Acov (X,Y)$<br>(3)  $cov(X+Y,Z) = cov (X,Z)+cov(Y,Z)$</p><h3 id="在线学习算法"><a href="#在线学习算法" class="headerlink" title="在线学习算法"></a>在线学习算法</h3><p>假设一个分布式数据挖掘系统拥有1个整合学习机和$\kappa$个本地学习机，$\kappa \in \{1,…,K\}$,时间线被分成离散的周期。在每一段周期n内，有一个数据实体<script type="math/tex">x^n = ((x_i^n)_{i=1}^d, y^n)</script> 输入系统， <script type="math/tex">(x_i^n)_{i=1}^d</script> 表示数据的属性特征，$y \in {Y}$是对这个实体的标记。对每一个到来的数据实体，每个本地学习机$k \in \kappa$都会观察属性特征集的一个子集<script type="math/tex">{A}_k \subseteq \{1,...,d\}</script>。 假设 <script type="math/tex">\bigcup_{k \in \kappa} {A}_k = \{1,...,d\}</script>,也就是说虽然单个本地学习机无法观察到数据的所有属性特征，但是所有本地学习机的观察集合可以。<br>在每个周期 n 内，对于$x^n$，每一个本地学习机，$k \in \kappa$,在基于观测 <script type="math/tex">{(x_j^n)}_{j \in A_k}</script> 的情况下, 会作出一个对 $y^n$的预测 $\hat{\mkern1mu y}_k^n$. 对于整个系统而言，最需要做的就是最大化整合学习机的预测准确性。为了实现这个目标，我们假设每个本地学习机 $k$ 都维护着有限个预测函数，定义为：</p><script type="math/tex; mode=display">\digamma_k = \{f : \prod_{j \in A_k} \chi_j \to R \}</script><p><script type="math/tex">\chi_j</script> 表示属性特征<script type="math/tex">x_j</script>的取值范围。每一个预测函数可以被理解为分类器（classifier）。在这，我们假设每个本地学习机用线性回归器（Linear regressor）将其所有可用的预测函数结合，然后去作预测。让<script type="math/tex">\vec{\mkern1mu f}_k = (f_{k1},...,f_{k|\digamma_k|})</script> 表示本地学习机 k 的所有预测函数。除此以外，本地学习机还维护着一个对每个预测函数的权重向量<script type="math/tex">\vec{\mkern1mu b}_k \in R^{|\digamma_k|}</script>, 所以在周期 n 内，本地学习机的预测通过 <script type="math/tex">\hat{\mkern1mu y}_k^n = \langle \vec{\mkern1mu b}_k, \vec{\mkern1mu f}_k({(x_j^n)}_{j \in A_k}) \rangle</script>得到.<br>Next整合学习机聚集所有本地学习机的预测，并用一个线性回归器作为整合学习机的预测函数去产生最后的输出：<script type="math/tex">\hat{\mkern1mu y}^n = \sum_{k \in \kappa}w_k \hat{\mkern1mu y}_k^n</script>. 向量<script type="math/tex">\vec{\mkern1mu w} = \{w_k\}_{k=1}^K</script> 是整合学习机赋值给本地学习机的权重向量。在这里，我们假设所有本地学习机的预测都是无偏向的(unbiased)。因此，所有权重之和标准化后为1:<script type="math/tex">\sum_{k=1}^K w_k =1</script>.<br>我们的目标是设计一个算法：对学习机权重进行设限，最小化实体预测值的平方差，也就是：</p><p><script type="math/tex">{\min \limits_{b_k}}\quad{\min \limits_w} \sum_{m=1}^n (y^m-\sum_{k=1}^K w_k \langle \vec{\mkern1mu b}_k,\vec{\mkern1mu f}_k({(x_j^m)}_{j \in A_k})\rangle)^2,\quad s.t. {||W||}_1 = 1, {||b_k||}_2 \le \lambda_k,\forall k \in \kappa</script>.<br>为了解决上式的问题，我们提出了一个更新整合学习机权重<script type="math/tex">\vec{\mkern1mu w}</script>,以及对每个本地学习机向量<script type="math/tex">\vec {\mkern1mu b}_k</script>的方法。 当<script type="math/tex">{(b_k^n)}_{k=1}^K</script>固定不变时，我们可以更新整合学习机的<script type="math/tex">\vec{\mkern1mu w}</script>向量。更新此向量就要解决下面的问题：<br>\begin{equation}<br>\min \limits_w {||y^n - \vec{\mkern1mu G^n} \vec{\mkern1mu w}||}_2^2, \quad s.t.{||\vec{\mkern1mu w}||}_1 = 1<br>\end{equation}</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1.<a href="https://www.zhihu.com/question/22246294" target="_blank" rel="noopener">https://www.zhihu.com/question/22246294</a><br>2.Zhang Y, Sow D, Turaga D, et al. A fast online learning algorithm for distributed mining of bigdata[J]. ACM SIGMETRICS Performance Evaluation Review, 2014, 41(4): 90-93.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;数据异质性&quot;&gt;&lt;a href=&quot;#数据异质性&quot; class=&quot;headerlink&quot; title=&quot;数据异质性&quot;&gt;&lt;/a&gt;数据异质性&lt;/h3&gt;&lt;p&gt;假设自变量是X，因变量是Y。那么如果要识别X是如何影响Y的，则要保证其他因素（非观测因素）不变。为了逻辑清晰可见，把非观测因素的作用归结为Z。如果X的值变化了，Z的值也随之变动，此时你观测到的Y的变动，到底是X引起的，还是Z引起的？&lt;br&gt;如果其他因素Z不变，则其可以被看作是误差项，并认为这个误差项的方差应该是保持不变的，这样的就叫“同方差”。但是如果Z随X的变化而变化，那么Z的方差就不是不变的，也就是说误差项Z表现出“异方差”特性，也就是数据的”数据异质性“。&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="http://memicat.com/categories/Data-Mining/"/>
    
    
      <category term="在线学习算法" scheme="http://memicat.com/tags/%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="Online learning algorithm" scheme="http://memicat.com/tags/Online-learning-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>流数据的无监督实时异常检测</title>
    <link href="http://memicat.com/2018/02/20/2018.02.20_%E6%B5%81%E6%95%B0%E6%8D%AE%E7%9A%84%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AE%9E%E6%97%B6%E5%BC%82%E5%B8%B8%E7%9B%91%E6%B5%8B/"/>
    <id>http://memicat.com/2018/02/20/2018.02.20_流数据的非监督实时异常监测/</id>
    <published>2018-02-19T23:00:00.000Z</published>
    <updated>2018-03-04T19:56:53.503Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>异常检测在工业实时流数据中非常重要，例如预防性维护，防欺诈，故障检测和监控（Preventative maintenance, fraud prevention, fault detection, and monitoring）等。异常可能是暂时性的（Temporal anomaly），暂时性异常可以预先发现潜在的问题，但是这种异常往往很难在实时数据流中被检测到，所以检测暂时性异常非常重要.<br><a id="more"></a><br>Batch processing, data is split into train/test sets, and algorithms cannot look ahead, but data streaming is not. 流数据处理（Data streaming）和批处理(Batch processing)不同，流数据处理是处理源源不断的数据流，而批处理是处理整个数据集 (full dataset)。  </p><h4 id="1-1-Streaming-applications"><a href="#1-1-Streaming-applications" class="headerlink" title="1.1 Streaming applications"></a>1.1 Streaming applications</h4><p>Streaming applications impose unique constraints and challenges for machine learning models.<br>The concept drift means that the statistical properties of the target variable, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.<br>Let the vector $\chi_t$ represent the state of a real-time system at time $t$.</p><script type="math/tex; mode=display">...,\chi_{t-2},\chi_{t-1},\chi_{t},\chi_{t+1},\chi_{t+2},...</script><p>There is a tradeoff between early detections and false positives, as an algorithm that makes frequent inaccurate detections is likely to be ignored.<br>We define the ideal characteristics of the real-world anomaly detection algorithm as follows:<br>1.Predictions must be made online; i.e., the algorithm must identify state <script type="math/tex">\chi_t</script> as normal or anomalous before receiving the subsequent <script type="math/tex">\chi_{t+1}</script>.<br>2.The algorithm must learn continuously without a requirement to store the entire stream.<br>3.The algorithm must run in an unsupervised, automated fashion,i.e., without data labels or manual parameter tweaking.<br>4.Algorithms must adapt to dynamic environments and concept drift, as the underlying statistics of the data stream is often non-stationary.<br>5.Algorithms should make anomaly detections as early as possible.<br>6.Algorithms should minimize false positives and false negatives (this is true for batch scenarios as well).</p><h4 id="1-2-Related-work"><a href="#1-2-Related-work" class="headerlink" title="1.2 Related work"></a>1.2 Related work</h4><p>Many anomaly detection approaches exist: Both supervised (e.g. support vector machines and decision trees) and unsupervised (e.g. clustering), yet many anomaly detection methods are for processing data in batches, and unsuitable for real-time streaming applications.<br>Some anomaly detection algorithms are partially online. They either have an initial phase of offline learning, or rely on look-ahead to flag previously-seen anomalous data.</p><h3 id="2-Anomaly-detection-using-HTM"><a href="#2-Anomaly-detection-using-HTM" class="headerlink" title="2. Anomaly detection using HTM"></a>2. Anomaly detection using HTM</h3><h4 id="2-1-Overview-of-HTM"><a href="#2-1-Overview-of-HTM" class="headerlink" title="2.1 Overview of HTM"></a>2.1 Overview of HTM</h4><p>Sparse encoding 稀疏编码  Disambiguate sequences 消歧序列<br>Tail probability：It is the probability that a variable will be larger than x standard deviations above the mean. 换句话说，尾部概率主要关注的是那些偏离平均值的事件发生的概率，一般来说，尾部概率通常特别关注那些远离好几倍误差的事件发生可能性。<br><img src="/images/2018.2.19_1.png" alt="" title="Fig. 1. (a) A block diagram outlining the primary functional steps used to create a complete anomaly detection system based on HTM. Our process takes the output of an HTM system and then performs two additional post-processing steps: computing the prediction error followed by computing an anomaly likelihood measure. (b) Breakdown of the core algorithm components within an HTM system."><br>The current input <script type="math/tex">\chi_t</script>, is fed to an encoder and then a sparse spatial pooling process. The resulting vector,<script type="math/tex">a(\chi_t)</script>, is a sparse binary vector representing the current input. The heart of the system is the sequence memory component. This component models temporal patterns in <script type="math/tex">a(\chi_t)</script> and outputs a prediction in the form of another sparse vector <script type="math/tex">π(\chi_{t})</script>. <script type="math/tex">π(\chi_{t})</script> is thus a prediction for <script type="math/tex">a(\chi_{t+1})</script>.<br><img src="/images/2018.2.19_2.png" alt="" title="Fig. 2. The HTM sequence memory. (a) HTM sequence memory models one layer of cortex. The layer consists of a set of mini-columns, with each mini-column containing multiple neurons. (b) An HTM neuron (left) models the dendritic structure of pyramidal neurons in cortex (right). An HTM neuron models dendrites as an array of coincident detectors, each with a set of synapses. Context dendrites receive lateral input from other neurons within the layer. Each dendrite represents one transition in a sequence. Sufficient lateral activity on a context dendrite will cause the cell to enter a predicted state. (c) Representing high-order Markov sequences with shared subsequences (ABCD vs. XBCY). Each sequence element invokes a sparse set of cells within mini-columns. Cells that are predicted through lateral connections prevent other cells in the same column from firing through intra-column inhibition resulting in a highly sparse representation. As shown in the figure, such a representation can maintain past context. Because different cells respond to “C” in the two sequences (C’ and C’’), they can then invoke the correct prediction of either D or Y depending on the input from two time steps ago."><br>The figure shows how the sparse representations are used to rep- resent temporal patterns and disambiguate sequences with long term dependencies. When receiving the next input, the network uses the difference between predicted input and the actual input to update its synaptic connections. Learning happens at every time step but since the representations are highly sparse only a tiny percentage of the synapses are updated.</p><h4 id="2-2-Computing-the-prediction-error"><a href="#2-2-Computing-the-prediction-error" class="headerlink" title="2.2 Computing the prediction error"></a>2.2 Computing the prediction error</h4><p>We first compute a measure of prediction error, st. Then, using a probabilistic model of st, we compute Lt, a likelihood that the system is in an anomalous state.<br>Given the current input <script type="math/tex">\chi_t</script> , <script type="math/tex">a(\chi_t)</script> is a sparse encoding of the current input, and $π(\chi_{t-1})$ is the sparse vector representing the HTM network’s internal prediction of <script type="math/tex">a(\chi_t)</script>. The dimensionality of both vectors is equal to the number of columns in the HTM network (use a standard value of 2048 for the number of columns in our experiments). Let the prediction error, st, be a scalar value inversely proportional to the number of bits common between the actual and predicted binary vectors:</p><script type="math/tex; mode=display">s_t = 1- \frac{\pi(\chi_{t-1})\centerdot a(\chi_t)}{|a(\chi_t)|}</script><p>$|a(\chi_t)|$是标量，表示数据流<script type="math/tex">|a(\chi_t)|</script>中所有比特位为“1”的总个数.如果预测和当前状态的<script type="math/tex">a(\chi_t)</script>相匹配，那么<script type="math/tex">s_t</script>就是0。否则如果预测向量和当前状态向量是正交向量（也就是两个向量没有共同的比特位),那么<script type="math/tex">s_t</script>就为1.</p><h4 id="2-3-Computing-anomaly-likelihood"><a href="#2-3-Computing-anomaly-likelihood" class="headerlink" title="2.3 Computing anomaly likelihood"></a>2.3 Computing anomaly likelihood</h4><p>Model the distribution as a rolling normal distribution where the sample mean $\mu_t$ and $\sigma_t^2$, are continuously updated from previous error values as follows:</p><script type="math/tex; mode=display">\mu_t = \frac{\sum_{i=0}^{i=W-1}s_{t-i}}{W}</script><script type="math/tex; mode=display">\sigma_t^2 = \frac{\sum_{i=0}^{i=W-1}(s_{t-i}-\mu_t)^2}{W-1}</script><p>We then compute a recent short term average of prediction errors, and apply a threshold to the Gaussian tail probability (Q-function) to decide whether or not to declare an anomaly. We define the anomaly likelihood as the complement of the tail probability:</p><script type="math/tex; mode=display">L_t = 1 - Q(\frac{\mu_t^{\sim}-\mu_t}{\sigma_t})</script><p>where <script type="math/tex">\mu_t^{\sim} = \frac{\sum_{i=0}^{W^\prime-1}s_{t-i}}{W^\prime}</script><br>Q-function：<script type="math/tex">Pr\{X > x\} = Q(\frac{x-\mu}{\sigma})</script><br>$W^\prime$ here is a window for a short term moving average, where $W^\prime \lll W$, the duration for computing the distribution of prediction errors. We threshold $L_t$ based on a user-defined parameter <script type="math/tex">\epsilon</script> to report an anomaly:</p><script type="math/tex; mode=display">≡ L_t \ge 1 − \epsilon</script><p>In practice we use a generous value of $W=8000,W^\prime=10,\epsilon = 10^{-5}.$</p><h3 id="3-Evaluation-of-streaming-anomaly-detection-algorithms"><a href="#3-Evaluation-of-streaming-anomaly-detection-algorithms" class="headerlink" title="3.Evaluation of streaming anomaly detection algorithms"></a>3.Evaluation of streaming anomaly detection algorithms</h3><p>The Numenta Anomaly Benchmark (NAB) is a novel benchmark for evaluating algorithms for anomaly detection in streaming, online applications.<br>1.Provide a dataset of labeled data streams from real-world streaming applications.<br>2.Provide a scoring methodology and set of constraints designed for streaming applications.<br>3.Provide a controlled open repository for researchers to evaluate and compare anomaly detection algorithms for streaming applications.  </p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>Ahmad S, Lavin A, Purdy S, et al. Unsupervised real-time anomaly detection for streaming data[J]. Neurocomputing, 2017, 262: 134-147.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Introduction&quot;&gt;&lt;/a&gt;1. Introduction&lt;/h3&gt;&lt;p&gt;异常检测在工业实时流数据中非常重要，例如预防性维护，防欺诈，故障检测和监控（Preventative maintenance, fraud prevention, fault detection, and monitoring）等。异常可能是暂时性的（Temporal anomaly），暂时性异常可以预先发现潜在的问题，但是这种异常往往很难在实时数据流中被检测到，所以检测暂时性异常非常重要.&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="http://memicat.com/categories/Data-Mining/"/>
    
    
      <category term="Data streaming" scheme="http://memicat.com/tags/Data-streaming/"/>
    
      <category term="流数据" scheme="http://memicat.com/tags/%E6%B5%81%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Abnormal detection" scheme="http://memicat.com/tags/Abnormal-detection/"/>
    
      <category term="异常检测" scheme="http://memicat.com/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>最大似然估计(Maximum likelihood estimation)</title>
    <link href="http://memicat.com/2018/02/14/2018.02.14_%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    <id>http://memicat.com/2018/02/14/2018.02.14_最大似然估计/</id>
    <published>2018-02-13T23:00:00.000Z</published>
    <updated>2018-02-22T10:44:19.112Z</updated>
    
    <content type="html"><![CDATA[<p>最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。最大似然估计中的采样需要满足假设：所有的采样都是独立分布的。<br><a id="more"></a><br>首先，假设$x_1,x_2,……x_n$为独立同分布的采样，$\theta$为模型参数，$f$为我们所使用的模型，遵循独立同步假设。那么参数为$\theta$的模型f可表示为：</p><script type="math/tex; mode=display">f(x_1,x_2,...,x_n|\theta) = f(x_1|\theta) \times f(x_2|\theta),...,f(x_n|\theta)</script><p>模型已定，参数未知：已知的为$x_1,x_2,……x_n$，未知的为$\theta$, 故似然定义为：</p><script type="math/tex; mode=display">L(\theta|x_1,x_2,...,x_n) = f(x_1,...,x_n|\theta) = \prod_{i=1}^nf(x_i|\theta)</script><p>两边取对数，得到：</p><script type="math/tex; mode=display">InL(\theta|x_1,...x_n) = \sum_{i=1}^nInf(x_i|\theta),假设 \ell_{avg} = \frac{1}{n}InL</script><p>其中的<script type="math/tex">InL(\theta|x_1,...x_n)</script>称为对数似然，而 $\ell_{avg}$为平均对数似然。平时接触最多的最大似然其实是最大的对数平均似然，即：</p><script type="math/tex; mode=display">\theta^{max}= max\ell_{avg}(\theta|x_1,...,x_n)</script><p>例如，一个袋子里装有黑白两种球，球的数量不知道，做100次放回性取球抽样实验，记录下每次取球的颜色。假设100次实验中，取得白球的概率是70%，黑球30%，则一般认为白球占总数的70%。虽然可以这么说，但这是一个直接的带有感官的答案，缺乏理论证明。接下来对其进行证明：<br>假设白球的所占比是p,那么黑球就是1-p. M是所给的模型，<script type="math/tex">Data = \{x_1,x_2,...,x_{100}\}</script> 是这一百次抽样结果的数据，则：</p><script type="math/tex; mode=display">P(Data|M) = P(x_1,x_2,...,x_{100}|M) = P(x_1|M) P(x_2|M)...P(x_{100}|M)=p^{70}(1-p)^{30}</script><p>那么，从基本的数学知识可知，要想找到这个结果的最大值，应对其进行求导，并使其导数等于0. 即：<script type="math/tex">70p^{69}\times(1-p)^{30}-p^{70}\times(1-p)^{29}=0</script>, 解得p=0.7. 也就是说白球所占比为0.7的可能性最大。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html" target="_blank" rel="noopener">https://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。最大似然估计中的采样需要满足假设：所有的采样都是独立分布的。&lt;br&gt;
    
    </summary>
    
      <category term="Math" scheme="http://memicat.com/categories/Math/"/>
    
    
      <category term="最大似然估计" scheme="http://memicat.com/tags/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="Maximum likelihood estimation" scheme="http://memicat.com/tags/Maximum-likelihood-estimation/"/>
    
  </entry>
  
  <entry>
    <title>Hexo常用数学公式</title>
    <link href="http://memicat.com/2018/02/14/2018.02.14_Hexo%20%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
    <id>http://memicat.com/2018/02/14/2018.02.14_Hexo 数学公式/</id>
    <published>2018-02-13T23:00:00.000Z</published>
    <updated>2018-03-06T20:58:13.868Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录我在写博客期间遇到的一些常用的及特殊的数学公式，怕自己忘了，所以记录下来怕自己忘记。<br><a id="more"></a></p><h3 id="常见数学符号"><a href="#常见数学符号" class="headerlink" title="常见数学符号"></a>常见数学符号</h3><p>$\theta$: \theta<br>$\times$: \times<br>$\prod$: \prod<br>$\pm$: \pm<br>$\mp$: \mp<br>$\ell$: \ell<br>$\digamma:$ \digamma<br>$\hat{\mkern1mu a}:$ \hat{\mkern1mu a}<br>$\to:$ \to<br>$\langle:$ \langle<br>$\rangle:$ \rangle<br>$\min \limits_w:$\min \limits_w<br>$\lambda:$ \lambda<br>$\forall:$ \forall<br>$\exists:$ \exists<br>$\subseteqq:$\subseteqq<br>$\in:$\in<br>$\bar {\mkern1mu W_k}:$\bar {\mkern1mu W_k}<br>$\Rightarrow:$\Rightarrow<br>$\rightarrow:$\rightarrow<br>$\infty:$\infty<br>$\succsim:$\succsim<br>$\precsim:$\precsim<br>$\succapprox:$\succapprox<br>$\precapprox:$\precapprox<br>$\succ:$\succ<br>$\prec:$\prec<br>$\beta:$\beta<br>$\theta:$\theta<br>$\zeta:$\zata<br>ξ:ξ<br>$\Omega:$\Omega:<br>$\omege:$\omege:<br>$\sim:$\sim<br>$\mathcal{L}:$\mathcal{L}<br>$\cdot:$\cdot<br>$\lmoustache:$\lmoustache<br>$\int:$\int<br>*</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="http://mohu.org/info/symbols/symbols.htm" target="_blank" rel="noopener">http://mohu.org/info/symbols/symbols.htm</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录我在写博客期间遇到的一些常用的及特殊的数学公式，怕自己忘了，所以记录下来怕自己忘记。&lt;br&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://memicat.com/categories/Hexo/"/>
    
    
      <category term="Math formul" scheme="http://memicat.com/tags/Math-formul/"/>
    
  </entry>
  
  <entry>
    <title>Hexo常用排版格式</title>
    <link href="http://memicat.com/2018/02/14/2018.02.14_Hexo%20%E6%8E%92%E7%89%88%E6%A0%BC%E5%BC%8F/"/>
    <id>http://memicat.com/2018/02/14/2018.02.14_Hexo 排版格式/</id>
    <published>2018-02-13T23:00:00.000Z</published>
    <updated>2018-03-06T21:09:47.913Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录我在写博客期间遇到的一些排版格式，怕自己忘了，所以记录下来怕自己忘记。<br><a id="more"></a></p><h3 id="1-基本语法"><a href="#1-基本语法" class="headerlink" title="1. 基本语法"></a>1. 基本语法</h3><h4 id="1-1-字体"><a href="#1-1-字体" class="headerlink" title="1.1 字体"></a>1.1 字体</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这是正常</span><br><span class="line">**这是粗体**</span><br><span class="line">***这是加粗斜体***</span><br><span class="line">*这是斜体* 或 _这也是斜体_</span><br><span class="line">~~这是删除线~~</span><br></pre></td></tr></table></figure><p>这是正常<br><strong>这是粗体</strong><br><strong><em>这是加粗斜体</em></strong><br><em>这是斜体</em> 或 <em>这也是斜体</em><br><del>这是删除线</del></p><h4 id="1-2-列表"><a href="#1-2-列表" class="headerlink" title="1.2 列表"></a>1.2 列表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* 无序列表项1</span><br><span class="line">+ 无序列表项2</span><br><span class="line">- 无序列表项3</span><br></pre></td></tr></table></figure><ul><li>无序列表项1</li></ul><ul><li>无序列表项2</li></ul><ul><li>无序列表项3</li></ul><h4 id="1-3-表格"><a href="#1-3-表格" class="headerlink" title="1.3 表格"></a>1.3 表格</h4><div class="table-container"><table><thead><tr><th style="text-align:center">组别</th><th style="text-align:center">属于娱乐</th><th style="text-align:center">不属于娱乐</th><th style="text-align:center">合计</th></tr></thead><tbody><tr><td style="text-align:center">不包含吴亦凡</td><td style="text-align:center">19</td><td style="text-align:center">24</td><td style="text-align:center">43</td></tr><tr><td style="text-align:center">包含吴亦凡</td><td style="text-align:center">34</td><td style="text-align:center">10</td><td style="text-align:center">44</td></tr><tr><td style="text-align:center">合计</td><td style="text-align:center">53</td><td style="text-align:center">34</td><td style="text-align:center">87</td></tr></tbody></table></div><h4 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$$  </span><br><span class="line">F_\theta=</span><br><span class="line">\left(</span><br><span class="line">  \begin&#123;array&#125;&#123;ccc&#125;</span><br><span class="line">  1 &amp; x_&#123;n1&#125;&amp; \cdots x_&#123;1k&#125; \\</span><br><span class="line">  \vdots &amp; \vdots &amp; \ddots \vdots \\</span><br><span class="line">  1 &amp; x_&#123;n2&#125; &amp; \cdots x_&#123;nk&#125; \\</span><br><span class="line">  \end&#123;array&#125;</span><br><span class="line">\right)</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">F_\theta=\left(  \begin{array}{ccc}  1 & x_{n1}& \cdots x_{1k} \\  \vdots & \vdots & \ddots \vdots \\  1 & x_{n2} & \cdots x_{nk} \\  \end{array}\right)</script><h3 id="2-公式插入"><a href="#2-公式插入" class="headerlink" title="2. 公式插入"></a>2. 公式插入</h3><p>公式加粗：</p><script type="math/tex; mode=display">a+b = \boldsymbol{a+b}</script><h4 id="2-1-多行赋值"><a href="#2-1-多行赋值" class="headerlink" title="2.1 多行赋值"></a>2.1 多行赋值</h4><p>双\\ 公式内换行，cases实现大括号右多行赋值，&amp;用来对齐。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">f(n) =</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">n/2,  &amp; \text&#123;if $n$ is even&#125; \\</span><br><span class="line">3n+1, &amp; \text&#123;if $n$ is odd&#125;</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure></p><script type="math/tex; mode=display">f(n) =\begin{cases}n/2,  & \text{if $n$ is even} \\3n+1, & \text{if $n$ is odd}\end{cases}</script><h4 id="2-2-多行公式对齐"><a href="#2-2-多行公式对齐" class="headerlink" title="2.2 多行公式对齐"></a>2.2 多行公式对齐</h4><p>begin{split} 表示开始多行公式，end{split}表示结束；公式中用\\表示回车到下一行，&amp;表示对齐的位置。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">\frac&#123;\partial^2 f&#125;&#123;\partial&#123;x^2&#125;&#125; &amp;= \frac&#123;\partial(\Delta_x f(i,j))&#125;&#123;\partial x&#125; = \frac&#123;\partial(f(i+1,j)-f(i,j))&#125;&#123;\partial x&#125; \\</span><br><span class="line">&amp;= \frac&#123;\partial f(i+1,j)&#125;&#123;\partial x&#125; - \frac&#123;\partial f(i,j)&#125;&#123;\partial x&#125; \\</span><br><span class="line">&amp;= f(i+2,j) -2f(f+1,j) + f(i,j)</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">\nonumber</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure></p><script type="math/tex; mode=display">\begin{equation}\begin{split}\frac{\partial^2 f}{\partial{x^2}} &= \frac{\partial(\Delta_x f(i,j))}{\partial x} = \frac{\partial(f(i+1,j)-f(i,j))}{\partial x} \\&= \frac{\partial f(i+1,j)}{\partial x} - \frac{\partial f(i,j)}{\partial x} \\&= f(i+2,j) -2f(f+1,j) + f(i,j)\end{split}\nonumber\end{equation}</script><h4 id="2-3-自动编号"><a href="#2-3-自动编号" class="headerlink" title="2.3 自动编号"></a>2.3 自动编号</h4><p>使用begin{equation} and end{equation}环境就会自动编号:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\sum_&#123;i=0&#125;^n F_i \cdot \phi (H, p_i) - \sum_&#123;i=1&#125;^n a_i \cdot ( \tilde&#123;x_i&#125;, \tilde&#123;y_i&#125;) + b_i \cdot ( \tilde&#123;x_i&#125;^2 , \tilde&#123;y_i&#125;^2 )</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure></p><script type="math/tex; mode=display">\begin{equation}\sum_{i=0}^n F_i \cdot \phi (H, p_i) - \sum_{i=1}^n a_i \cdot ( \tilde{x_i}, \tilde{y_i}) + b_i \cdot ( \tilde{x_i}^2 , \tilde{y_i}^2 )\end{equation}</script><h4 id="2-4-禁止自动编号"><a href="#2-4-禁止自动编号" class="headerlink" title="2.4 禁止自动编号"></a>2.4 禁止自动编号</h4><p>在end{equation}前加\nonumber可禁止对此公式自动编号，例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\sum_&#123;i=0&#125;^n F_i \cdot \phi (H, p_i) - \sum_&#123;i=1&#125;^n a_i \cdot ( \tilde&#123;x_i&#125;, \tilde&#123;y_i&#125;) + b_i \cdot ( \tilde&#123;x_i&#125;^2 , \tilde&#123;y_i&#125;^2 )</span><br><span class="line">\nonumber</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\beta^*(D) = \mathop&#123;argmin&#125; \limits_&#123;\beta&#125; \lambda &#123;||\beta||&#125;^2 + \sum_&#123;i=1&#125;^n max(0, 1 - y_i f_&#123;\beta&#125;(x_i))</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure></p><script type="math/tex; mode=display">\begin{equation}\sum_{i=0}^n F_i \cdot \phi (H, p_i) - \sum_{i=1}^n a_i \cdot ( \tilde{x_i}, \tilde{y_i}) + b_i \cdot ( \tilde{x_i}^2 , \tilde{y_i}^2 )\nonumber\end{equation}</script><script type="math/tex; mode=display">\begin{equation}\beta(D) = \mathop{argmin} \limits_{\beta} \lambda {||\beta||}^2 + \sum_{i=1}^n max(0, 1 - y_i f_{\beta}(x_i))\end{equation}</script><h4 id="2-5-公式手动编号："><a href="#2-5-公式手动编号：" class="headerlink" title="2.5 公式手动编号："></a>2.5 公式手动编号：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\sum_&#123;i=0&#125;^n F_i \cdot \phi (H, p_i) - \sum_&#123;i=1&#125;^n a_i \cdot ( \tilde&#123;x_i&#125;, \tilde&#123;y_i&#125;) + b_i \cdot ( \tilde&#123;x_i&#125;^2 , \tilde&#123;y_i&#125;^2 ) \tag&#123;1.2.3&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation}\sum_{i=0}^n F_i \cdot \phi (H, p_i) - \sum_{i=1}^n a_i \cdot ( \tilde{x_i}, \tilde{y_i}) + b_i \cdot ( \tilde{x_i}^2 , \tilde{y_i}^2 ) \tag{1.2.3}\end{equation}</script><h4 id="2-6-行间公式编号"><a href="#2-6-行间公式编号" class="headerlink" title="2.6  行间公式编号"></a>2.6  行间公式编号</h4><p>行内公式加\tag{}后会自动成为行间公式，例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ s = r cos(a+b) = r cos(a) cos(b) - r sin(a) sin(b) \tag&#123;1.1&#125; $</span><br><span class="line">$ t = r sin(a+b) = r sin(a) cos(b) - r cos(a) sin(b) \tag&#123;1.2&#125; $</span><br></pre></td></tr></table></figure></p><p>$ s = r cos(a+b) = r cos(a) cos(b) - r sin(a) sin(b) \tag{1.1} $<br>$ t = r sin(a+b) = r sin(a) cos(b) - r cos(a) sin(b) \tag{1.2} $</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录我在写博客期间遇到的一些排版格式，怕自己忘了，所以记录下来怕自己忘记。&lt;br&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://memicat.com/categories/Hexo/"/>
    
    
      <category term="Latex排版" scheme="http://memicat.com/tags/Latex%E6%8E%92%E7%89%88/"/>
    
      <category term="Hexo排版" scheme="http://memicat.com/tags/Hexo%E6%8E%92%E7%89%88/"/>
    
  </entry>
  
  <entry>
    <title>从这说起</title>
    <link href="http://memicat.com/2018/01/30/2018.01.30_%E4%BB%8E%E8%BF%99%E8%AF%B4%E8%B5%B7/"/>
    <id>http://memicat.com/2018/01/30/2018.01.30_从这说起/</id>
    <published>2018-01-29T23:00:00.000Z</published>
    <updated>2018-03-04T19:56:50.026Z</updated>
    
    <content type="html"><![CDATA[<p>第一次有想写博客的想法其实是在2017年12月份，然后去英国过了个圣诞，回来后学习热情消失殆尽，故一直拖到如今，有几个原因促成这个博客的诞生：<br><a id="more"></a><br>1.自己虽大学毕业了， 但是让自己总结大学四年学到了什么专业知识，我竟一时语塞；甚至让我回想一些对自己影响颇深的人和事，记住的也是寥寥无几。古人说，“好记性不如烂笔头”，用在这再贴合不过了。人的记忆有限，况且时代变化还这么快，想要通过文字来记录自己的所学，所想，所感！<br>2.身边有不少朋友经营着自己的微信公众号，有分享自己摄影作品的，有分享自己所感所想的，甚至还有和商业合作专门写软文的，发展兴趣的同时赚点外快，有点心生羡慕，公众号的发展也让有好文采的人能够大展身手.<br>3.关于我为什么不选择开通一个微信公众号，那当然是觉得自己的文字能力有限TT,同时我想要的是一个远离社交圈的灵魂栖息地，一个真正属于自己的小小空间，可以在这肆意发疯，记下自己的碎碎念，见证自己的成长。</p><p>博客涵盖内容：计算机相关 投资理财 鸡汤</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第一次有想写博客的想法其实是在2017年12月份，然后去英国过了个圣诞，回来后学习热情消失殆尽，故一直拖到如今，有几个原因促成这个博客的诞生：&lt;br&gt;
    
    </summary>
    
      <category term="My life" scheme="http://memicat.com/categories/My-life/"/>
    
    
      <category term="博客" scheme="http://memicat.com/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
