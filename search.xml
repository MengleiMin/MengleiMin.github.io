<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大数据分布式挖掘的在线学习算法]]></title>
    <url>%2F2018%2F02%2F21%2F2018.02.21_%E5%A4%A7%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1.基础知识1.1 数据异质性假设自变量是X，因变量是Y。那么如果要识别X是如何影响Y的，则要保证其他因素（非观测因素）不变。为了逻辑清晰可见，把非观测因素的作用归结为Z。如果X的值变化了，Z的值也随之变动，此时你观测到的Y的变动，到底是X引起的，还是Z引起的？如果其他因素Z不变，则其可以被看作是误差项，并认为这个误差项的方差应该是保持不变的，这样的就叫“同方差”。但是如果Z随X的变化而变化，那么Z的方差就不是不变的，也就是说误差项Z表现出“异方差”特性，也就是数据的”数据异质性“。 Referencehttps://www.zhihu.com/question/22246294http://www.qiujiawei.com/linear-algebra-15/]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>Big data</tag>
        <tag>大数据</tag>
        <tag>Least Squares Estimator</tag>
        <tag>最小二乘估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小二乘估计（Least Squares Estimator）]]></title>
    <url>%2F2018%2F02%2F21%2F20180.02.21_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[最小二乘估计在Machine learning领域被广泛使用，接下来就来简单的介绍一下这个方法，首先，看一个式子： y=ax+e这是一个简单的直线方程，X是自变量，y是因变量，e是一个常量，此等式只能代表一个自变量对因变量的影响。如果我们把一个自变量拓展到n个自变量，比如影响一个人的智商的可能是年龄，但也有可能是环境，基因，受教育程度等。则以上式子就变为： y_i = a_1x_{i1} +...+ a_2x_{i2}+...+a_kx_{ik}+e_i, 1 \le i \le n, k\ge1换做矩阵表示为： \vec{\mkern1mu y} = X \vec{\mkern1mu a} + \vec{\mkern1mu e}\left[ \begin{array}{c} y1\\ y2\\ \vdots\\ y_n\\ \end{array} \right] = \left[ \begin{array}{cccc} x_{11} & x_{12} & \cdots x_{1k} \\ x_{21} & x_{22} & \cdots x_{2k} \\ \vdots & \vdots & \ddots \vdots \\ x_{n1} & x_{n2} & \cdots x_{nk} \\ \end{array} \right] \left[ \begin{array}{c} a1\\ a2\\ \vdots\\ a_k\\ \end{array} \right] +\left[ \begin{array}{c} e1\\ e2\\ \vdots\\ e_n\\ \end{array} \right]为了让$\vec{\mkern1mu a}$变得更有意义，那么就要使$\vec{\mkern1mu e}$尽量的小。也就是让$\vec{\mkern1mu e}$的长度尽量小。 |\vec{\mkern1mu e}| = \sqrt{\sum_{i=1}^n e_i^2}两边平方得： |\vec{\mkern1mu e}|^2 = \sum_{i=1}^n e_i^2 = \vec{\mkern1mu e}\vec{\mkern1mu e} = \vec{\mkern1mu e}^{T}\vec{\mkern1mu e}也就是说，当$\vec{\mkern1mu e}^{T}\vec{\mkern1mu e}$取最小值时，$\vec{\mkern1mu a}$能取得最优解。继续推倒，可以得到下式（证明略，有兴趣的可以看reference的第二个链接）： \vec{\mkern1mu e}^{T}\vec{\mkern1mu e} = \vec{\mkern1mu y}^{T}\vec{\mkern1mu y} - 2\vec{\mkern1mu y}^{T}X\vec{\mkern1mu a}+\vec{\mkern1mu a}^{T}X^{T}X\vec{\mkern1mu a}然后利用矩阵微分，可得： \vec{\mkern1mu a} = (X^TX)^{-1}X^T\vec{\mkern1mu y}这个式子就是最小二乘估计。 特殊情况下的最小二乘估计：如果X是一个方阵，则 \vec{\mkern1mu a} = X^{-1}\vec{\mkern1mu y}可以发现，当为方阵时，最小二乘估计很简单。不是方阵时，由于$X^{-1}$和$（X^T）^{-1}$都不成立，导致最小二乘估计无法继续化简。 Referencehttps://www.zhihu.com/question/22246294http://www.qiujiawei.com/linear-algebra-15/]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Least Squares Estimator</tag>
        <tag>最小二乘估计</tag>
        <tag>Math learning</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流数据的无监督实时异常检测]]></title>
    <url>%2F2018%2F02%2F20%2F2018.02.20_%E6%B5%81%E6%95%B0%E6%8D%AE%E7%9A%84%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AE%9E%E6%97%B6%E5%BC%82%E5%B8%B8%E7%9B%91%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[1. Introduction异常检测在工业实时流数据中非常重要，例如预防性维护，防欺诈，故障检测和监控（Preventative maintenance, fraud prevention, fault de- tection, and monitoring）等。异常可能是暂时性的（Temporal anomaly），暂时性异常可以预先发现潜在的问题，但是这种异常往往很难在实时数据流中被检测到，所以检测暂时性异常非常重要.Batch processing, data is split into train/test sets, and algorithms cannot look ahead, but data streaming is not. 流数据处理（Data streaming）和批处理(Batch processing)不同，流数据处理是处理源源不断的数据流，而批处理是处理整个数据集 (full dataset)。 1.1 Streaming applicationsStreaming applications impose unique constraints and challenges for machine learning models.The concept drift means that the statistical properties of the target variable, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.Let the vector $\chi_t$ represent the state of a real-time system at time $t$. ...,\chi_{t-2},\chi_{t-1},\chi_{t},\chi_{t+1},\chi_{t+2},...There is a tradeoff between early detections and false positives, as an algorithm that makes frequent inaccurate detections is likely to be ignored.We define the ideal characteristics of the real-world anomaly detection algorithm as follows:1.Predictions must be made online; i.e., the algorithm must identify state \chi_t as normal or anomalous before receiving the subsequent \chi_{t+1}.2.The algorithm must learn continuously without a requirement to store the entire stream.3.The algorithm must run in an unsupervised, automated fashion,i.e., without data labels or manual parameter tweaking.4.Algorithms must adapt to dynamic environments and concept drift, as the underlying statistics of the data stream is often non-stationary.5.Algorithms should make anomaly detections as early as possible.6.Algorithms should minimize false positives and false negatives (this is true for batch scenarios as well). 1.2 Related workMany anomaly detection approaches exist: Both supervised (e.g. support vector machines and decision trees) and unsupervised (e.g. clustering), yet many anomaly detection methods are for processing data in batches, and unsuitable for real-time streaming applications.Some anomaly detection algorithms are partially online. They either have an initial phase of offline learning, or rely on look-ahead to flag previously-seen anomalous data. 2. Anomaly detection using HTM2.1 Overview of HTMSparse encoding 稀疏编码 Disambiguate sequences 消歧序列Tail probability：It is the probability that a variable will be larger than x standard deviations above the mean. 换句话说，尾部概率主要关注的是那些偏离平均值的事件发生的概率，一般来说，尾部概率通常特别关注那些远离好几倍误差的事件发生可能性。The current input \chi_t, is fed to an encoder and then a sparse spatial pooling process. The resulting vector,a(\chi_t), is a sparse binary vector representing the current input. The heart of the system is the sequence memory component. This component models temporal patterns in a(\chi_t) and outputs a prediction in the form of another sparse vector π(\chi_{t}). π(\chi_{t}) is thus a prediction for a(\chi_{t+1}).The figure shows how the sparse representations are used to rep- resent temporal patterns and disambiguate sequences with long term dependencies. When receiving the next input, the network uses the difference between predicted input and the actual input to update its synaptic connections. Learning happens at every time step but since the representations are highly sparse only a tiny percentage of the synapses are updated. 2.2 Computing the prediction errorWe first compute a measure of prediction error, st. Then, using a probabilistic model of st, we compute Lt, a likelihood that the system is in an anomalous state.Given the current input \chi_t , a(\chi_t) is a sparse encoding of the current input, and $π(\chi_{t-1})$ is the sparse vector representing the HTM network’s internal prediction of a(\chi_t). The dimensionality of both vectors is equal to the number of columns in the HTM network (use a standard value of 2048 for the number of columns in our experiments). Let the prediction error, st, be a scalar value inversely proportional to the number of bits common between the actual and predicted binary vectors: s_t = 1- \frac{\pi(\chi_{t-1})\centerdot a(\chi_t)}{|a(\chi_t)|}$|a(\chi_t)|$是标量，表示数据流|a(\chi_t)|中所有比特位为“1”的总个数.如果预测和当前状态的a(\chi_t)相匹配，那么s_t就是0。否则如果预测向量和当前状态向量是正交向量（也就是两个向量没有共同的比特位),那么s_t就为1. 2.3 Computing anomaly likelihoodModel the distribution as a rolling normal distribution where the sample mean $\mu_t$ and $\sigma_t^2$, are continuously updated from previous error values as follows: \mu_t = \frac{\sum_{i=0}^{i=W-1}s_{t-i}}{W}\sigma_t^2 = \frac{\sum_{i=0}^{i=W-1}(s_{t-i}-\mu_t)^2}{W-1}We then compute a recent short term average of prediction errors, and apply a threshold to the Gaussian tail probability (Q-function) to decide whether or not to declare an anomaly. We define the anomaly likelihood as the complement of the tail probability: L_t = 1 - Q(\frac{\mu_t^{\sim}-\mu_t}{\sigma_t})where \mu_t^{\sim} = \frac{\sum_{i=0}^{W^\prime-1}s_{t-i}}{W^\prime}Q-function：Pr\{X > x\} = Q(\frac{x-\mu}{\sigma})$W^\prime$ here is a window for a short term moving average, where $W^\prime \lll W$, the duration for computing the distribution of prediction errors. We threshold $L_t$ based on a user-defined parameter \epsilon to report an anomaly: ≡ L_t \ge 1 − \epsilonIn practice we use a generous value of $W=8000,W^\prime=10,\epsilon = 10^{-5}.$ 3.Evaluation of streaming anomaly detection algorithmsThe Numenta Anomaly Benchmark (NAB) is a novel benchmark for evaluating algorithms for anomaly detection in streaming, online applications.1.Provide a dataset of labeled data streams from real-world streaming applications.2.Provide a scoring methodology and set of constraints designed for streaming applications.3.Provide a controlled open repository for researchers to evaluate and compare anomaly detection algorithms for streaming applications. ReferenceAhmad S, Lavin A, Purdy S, et al. Unsupervised real-time anomaly detection for streaming data[J]. Neurocomputing, 2017, 262: 134-147.]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>Data streaming</tag>
        <tag>流数据</tag>
        <tag>Abnormal detection</tag>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Mixed Models]]></title>
    <url>%2F2018%2F02%2F14%2FLinear%20mixed%20model%2F</url>
    <content type="text"><![CDATA[首先来区分Factor（因子） &amp; Covariates （协变量）：前者是名目变量，一般只含两至数个类别，每个类别至少有30个案例，后者是连续或定距变量（可以含成千上百个类别，每个类别中只含一至数个案例）。协变量一般用来指“控制变量”，可以说连续变量（如年龄）、也可以是名目变量（如性别）。Fixed factor: qualitative covariate，固定因子 = 定性协变量， 例如gender, agegroup.Fixed effect: quantitative covariate，定量协变量， 例如age.Random factor: qualitative variable whose levels are randomly sampled from a population of levels being studied.Random effect：quantitative variable whose levels are randomly sampled from a population of levels being studied. Referencehttp://zjz06.blogspot.se/2011/08/fixed-factorsrandom-factorscovariates.html]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>Linear Mixed Models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用排版格式]]></title>
    <url>%2F2018%2F02%2F14%2F2018.02.14_Hexo%20%E6%8E%92%E7%89%88%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[本文主要记录我在写博客期间遇到的一些排版格式，怕自己忘了，所以记录下来怕自己忘记。 基本语法字体加粗: **斜体: *]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Latex排版</tag>
        <tag>Hexo排版</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用数学公式]]></title>
    <url>%2F2018%2F02%2F14%2F2018.02.14_Hexo%20%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[本文主要记录我在写博客期间遇到的一些常用的及特殊的数学公式，怕自己忘了，所以记录下来怕自己忘记。 常见数学符号$\theta$: \theta$\times$: \times$\prod$: \prod$\pm$: \pm$\mp$: \mp$\ell$: \ell Referencehttp://mohu.org/info/symbols/symbols.htm]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Math formul</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大似然估计(Maximum likelihood estimation)]]></title>
    <url>%2F2018%2F02%2F14%2F2018.02.14_%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。最大似然估计中的采样需要满足假设：所有的采样都是独立分布的。首先，假设$x_1,x_2,……x_n$为独立同分布的采样，$\theta$为模型参数，$f$为我们所使用的模型，遵循独立同步假设。那么参数为$\theta$的模型f可表示为： f(x_1,x_2,...,x_n|\theta) = f(x_1|\theta) \times f(x_2|\theta),...,f(x_n|\theta)模型已定，参数未知：已知的为$x_1,x_2,……x_n$，未知的为$\theta$, 故似然定义为： L(\theta|x_1,x_2,...,x_n) = f(x_1,...,x_n|\theta) = \prod_{i=1}^nf(x_i|\theta)两边取对数，得到： InL(\theta|x_1,...x_n) = \sum_{i=1}^nInf(x_i|\theta),假设 \ell_{avg} = \frac{1}{n}InL其中的InL(\theta|x_1,...x_n)称为对数似然，而 $\ell_{avg}$为平均对数似然。平时接触最多的最大似然其实是最大的对数平均似然，即： \theta^{max}= max\ell_{avg}(\theta|x_1,...,x_n)例如，一个袋子里装有黑白两种球，球的数量不知道，做100次放回性取球抽样实验，记录下每次取球的颜色。假设100次实验中，取得白球的概率是70%，黑球30%，则一般认为白球占总数的70%。虽然可以这么说，但这是一个直接的带有感官的答案，缺乏理论证明。接下来对其进行证明：假设白球的所占比是p,那么黑球就是1-p. M是所给的模型，Data = \{x_1,x_2,...,x_{100}\} 是这一百次抽样结果的数据，则： P(Data|M) = P(x_1,x_2,...,x_{100}|M) = P(x_1|M) P(x_2|M)...P(x_{100}|M)=p^{70}(1-p)^{30}那么，从基本的数学知识可知，要想找到这个结果的最大值，应对其进行求导，并使其导数等于0. 即：70p^{69}\times(1-p)^{30}-p^{70}\times(1-p)^{29}=0, 解得p=0.7. 也就是说白球所占比为0.7的可能性最大。 Referencehttps://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>最大似然估计</tag>
        <tag>Maximum likelihood estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从这说起]]></title>
    <url>%2F2018%2F01%2F30%2F2018.01.30_%E4%BB%8E%E8%BF%99%E8%AF%B4%E8%B5%B7%2F</url>
    <content type="text"><![CDATA[第一次有想写博客的想法其实是在2017年12月份，然后去英国过了个圣诞，回来后学习热情消失殆尽，故一直拖到如今，有几个原因促成这个博客的诞生：1.自己虽大学毕业了， 但是让自己总结大学四年学到了什么专业知识，我竟一时语塞；甚至让我回想一些对自己影响颇深的人和事，记住的也是寥寥无几。古人说，“好记性不如烂笔头”，用在这再贴合不过了。人的记忆有限，况且时代变化还这么快，想要通过文字来记录自己的所学，所想，所感！2.身边有不少朋友经营着自己的微信公众号，有分享自己摄影作品的，有分享自己所感所想的，甚至还有和商业合作专门写软文的，发展兴趣的同时赚点外快，有点心生羡慕，公众号的发展也让有好文采的人能够大展身手.3.关于我为什么不选择开通一个微信公众号，那当然是觉得自己的文字能力有限TT,同时我想要的是一个远离社交圈的灵魂栖息地，一个真正属于自己的小小空间，可以在这肆意发疯，记下自己的碎碎念，见证自己的成长。 博客涵盖内容：计算机相关 投资理财 鸡汤]]></content>
      <categories>
        <category>My life</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
</search>
